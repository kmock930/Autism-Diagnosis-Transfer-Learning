{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.574674Z",
     "start_time": "2024-11-14T03:08:28.561142Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, losses, optimizers, metrics\n",
    "\n",
    "from model_train import (\n",
    "    load_c3d_model,\n",
    "    train_msupcl_model,\n",
    "    linear_evaluation,\n",
    "    load_c3d_sscl_model,\n",
    "    train_simclr_model,\n",
    "    linear_evaluation_sscl,\n",
    "    supervised_contrastive_loss,\n",
    "    nt_xent_loss,\n",
    ")\n",
    "from data_uniform_sup import VideoDataGenerator\n",
    "from data_uniform_sscl import SSCLVideoDataGenerator\n",
    "from paired_generator import PairedDataGenerator\n",
    "from model_train_r2plus1d_18 import (\n",
    "    load_r2plus1d_model,\n",
    "    load_sscl_r2plus1d_model,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.621916Z",
     "start_time": "2024-11-14T03:08:28.607885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 2042\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "input_shape = (12, 64, 64, 3)  # 数据生成器中定义的输入形状\n",
    "num_classes = 2  # 有害内容或安全内容\n",
    "feature_dim = 512\n",
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "temperature = 0.8\n",
    "learning_rate = 0.001\n"
   ],
   "id": "dd29972a569d4e84",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.636051Z",
     "start_time": "2024-11-14T03:08:28.624921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define dataset paths\n",
    "violence_negative_dir = './data/violence_dataset/NonViolence'\n",
    "violence_positive_dir = './data/violence_dataset/Violence'\n",
    "tiktok_negative_dir = './data/tiktok/train/Safe'\n",
    "tiktok_positive_dir = './data/tiktok/train/Harmful Content'\n"
   ],
   "id": "38b4e7f27f35b855",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.666733Z",
     "start_time": "2024-11-14T03:08:28.652138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义函数用于采样视频\n",
    "def sample_videos(directory, num_samples=100):\n",
    "    all_videos = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "        if f.endswith('.mp4')\n",
    "    ]\n",
    "    sampled_videos = random.sample(all_videos, min(num_samples, len(all_videos)))\n",
    "    return sampled_videos\n"
   ],
   "id": "69be9a7b8c19f6d5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.697287Z",
     "start_time": "2024-11-14T03:08:28.683768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Violence dataset\n",
    "violence_negative_videos = sample_videos(violence_negative_dir, 50)\n",
    "violence_positive_videos = sample_videos(violence_positive_dir, 50)\n",
    "\n",
    "# TikTok dataset\n",
    "tiktok_negative_videos = sample_videos(tiktok_negative_dir, 50)\n",
    "tiktok_positive_videos = sample_videos(tiktok_positive_dir, 50)\n"
   ],
   "id": "43cccf67e2dcc321",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.728950Z",
     "start_time": "2024-11-14T03:08:28.714416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data(negative_videos, positive_videos, train_ratio=0.55, val_ratio=0.15):\n",
    "    # 合并并打乱数据\n",
    "    videos = negative_videos + positive_videos\n",
    "    labels = [0] * len(negative_videos) + [1] * len(positive_videos)\n",
    "    combined = list(zip(videos, labels))\n",
    "    random.shuffle(combined)\n",
    "    videos[:], labels[:] = zip(*combined)\n",
    "\n",
    "    # 计算划分索引\n",
    "    total = len(videos)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "\n",
    "    # 划分数据集\n",
    "    train_videos = videos[:train_end]\n",
    "    train_labels = labels[:train_end]\n",
    "    val_videos = videos[train_end:val_end]\n",
    "    val_labels = labels[train_end:val_end]\n",
    "    test_videos = videos[val_end:]\n",
    "    test_labels = labels[val_end:]\n",
    "\n",
    "    return (train_videos, train_labels), (val_videos, val_labels), (test_videos, test_labels)\n",
    "\n"
   ],
   "id": "f8fa8efe1d13ebf5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.759925Z",
     "start_time": "2024-11-14T03:08:28.746039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Violence dataset\n",
    "(\n",
    "    (violence_train_videos, violence_train_labels),\n",
    "    (violence_val_videos, violence_val_labels),\n",
    "    (violence_test_videos, violence_test_labels),\n",
    ") = split_data(violence_negative_videos, violence_positive_videos)\n",
    "\n",
    "\n",
    "# TikTok dataset\n",
    "(\n",
    "    (tiktok_train_videos, tiktok_train_labels),\n",
    "    (tiktok_val_videos, tiktok_val_labels),\n",
    "    (tiktok_test_videos, tiktok_test_labels),\n",
    ") = split_data(tiktok_negative_videos, tiktok_positive_videos)\n"
   ],
   "id": "17bc85a97ca94a61",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.790604Z",
     "start_time": "2024-11-14T03:08:28.776153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Convert labels to numpy arrays and one-hot encode them if necessary\n",
    "def prepare_labels(labels):\n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "violence_train_labels_np = prepare_labels(violence_train_labels)\n",
    "violence_val_labels_np = prepare_labels(violence_val_labels)\n",
    "violence_test_labels_np = prepare_labels(violence_test_labels)\n",
    "\n",
    "\n",
    "tiktok_train_labels_np = prepare_labels(tiktok_train_labels)\n",
    "tiktok_val_labels_np = prepare_labels(tiktok_val_labels)\n",
    "tiktok_test_labels_np = prepare_labels(tiktok_test_labels)"
   ],
   "id": "d7db8f86a7e4e430",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.821279Z",
     "start_time": "2024-11-14T03:08:28.807249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Violence dataset generators\n",
    "violence_train_generator = VideoDataGenerator(\n",
    "    violence_train_videos,\n",
    "    violence_train_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=True,\n",
    ")\n",
    "violence_val_generator = VideoDataGenerator(\n",
    "    violence_val_videos,\n",
    "    violence_val_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "violence_test_generator = VideoDataGenerator(\n",
    "    violence_test_videos,\n",
    "    violence_test_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ],
   "id": "f150cbe36dce5ba1",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:28.852765Z",
     "start_time": "2024-11-14T03:08:28.838735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TikTok dataset generators\n",
    "tiktok_train_generator = VideoDataGenerator(\n",
    "    tiktok_train_videos,\n",
    "    tiktok_train_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=True,\n",
    ")\n",
    "tiktok_val_generator = VideoDataGenerator(\n",
    "    tiktok_val_videos,\n",
    "    tiktok_val_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "tiktok_test_generator = VideoDataGenerator(\n",
    "    tiktok_test_videos,\n",
    "    tiktok_test_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n"
   ],
   "id": "b129d04af287a968",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:29.080644Z",
     "start_time": "2024-11-14T03:08:28.868711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "base_model = load_c3d_model(input_shape=input_shape, feature_dim=feature_dim)\n",
    "print(\"Base Model Summary:\")\n",
    "base_model.summary()"
   ],
   "id": "b4c77a016021fe8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Summary:\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 12, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 12, 64, 64, 64)    5248      \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 12, 32, 32, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 12, 32, 32, 128)   221312    \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 6, 16, 16, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 6, 16, 16, 256)    884992    \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 6, 16, 16, 256)    1769728   \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 6, 8, 8, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_12 (Conv3D)          (None, 6, 8, 8, 512)      3539456   \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 6, 8, 8, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 3, 4, 4, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 3, 2, 2, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              25169920  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,704,832\n",
      "Trainable params: 71,704,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:29.108509Z",
     "start_time": "2024-11-14T03:08:29.097121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_classification_model(base_model, num_classes):\n",
    "    features = base_model.output\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(features)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model"
   ],
   "id": "f3d652978d3df6f9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:29.156762Z",
     "start_time": "2024-11-14T03:08:29.125540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classification_model_violence = create_classification_model(base_model, num_classes)\n",
    "# 冻结基础模型的参数\n",
    "for layer in classification_model_violence.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "classification_model_violence.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
    ")\n"
   ],
   "id": "600a86c162f46bac",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:29.203382Z",
     "start_time": "2024-11-14T03:08:29.172879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Classification Model for Violence Dataset Summary:\")\n",
    "classification_model_violence.summary()"
   ],
   "id": "bdd247846a39c1f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Model for Violence Dataset Summary:\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 12, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 12, 64, 64, 64)    5248      \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 12, 32, 32, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 12, 32, 32, 128)   221312    \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 6, 16, 16, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 6, 16, 16, 256)    884992    \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 6, 16, 16, 256)    1769728   \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 6, 8, 8, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_12 (Conv3D)          (None, 6, 8, 8, 512)      3539456   \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 6, 8, 8, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 3, 4, 4, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 3, 2, 2, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              25169920  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,705,858\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 71,704,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:53.586516Z",
     "start_time": "2024-11-14T03:08:29.219908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_violence = classification_model_violence.fit(\n",
    "    violence_train_generator,\n",
    "    validation_data=violence_val_generator,\n",
    "    epochs=num_epochs,\n",
    "    \n",
    ")"
   ],
   "id": "80ad2bb7d4358a76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13/13 [==============================] - 9s 637ms/step - loss: 0.6935 - sparse_categorical_accuracy: 0.4712 - val_loss: 0.6935 - val_sparse_categorical_accuracy: 0.4167\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 8s 620ms/step - loss: 0.6928 - sparse_categorical_accuracy: 0.5577 - val_loss: 0.6935 - val_sparse_categorical_accuracy: 0.4167\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 8s 580ms/step - loss: 0.6927 - sparse_categorical_accuracy: 0.5192 - val_loss: 0.6937 - val_sparse_categorical_accuracy: 0.4167\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:58.057513Z",
     "start_time": "2024-11-14T03:08:53.604550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on Violence test set\n",
    "base_c3d_results_violence = classification_model_violence.evaluate(violence_test_generator)\n",
    "print(\n",
    "    f\"Violence Dataset - Test Loss: {base_c3d_results_violence[0]}, Test Accuracy: {base_c3d_results_violence[1]}\"\n",
    ")"
   ],
   "id": "63b234494c99c2b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 4s 492ms/step - loss: 0.6929 - sparse_categorical_accuracy: 0.5357\n",
      "Violence Dataset - Test Loss: 0.6929368376731873, Test Accuracy: 0.5357142686843872\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:08:58.120519Z",
     "start_time": "2024-11-14T03:08:58.076549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classification_model_tiktok = create_classification_model(base_model, num_classes)\n",
    "\n",
    "for layer in classification_model_tiktok.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "classification_model_tiktok.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "print(\"Classification Model for TikTok Dataset Summary:\")\n",
    "classification_model_tiktok.summary()"
   ],
   "id": "df01412c8d97718e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Model for TikTok Dataset Summary:\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 12, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 12, 64, 64, 64)    5248      \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 12, 32, 32, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 12, 32, 32, 128)   221312    \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 6, 16, 16, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 6, 16, 16, 256)    884992    \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 6, 16, 16, 256)    1769728   \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 6, 8, 8, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_12 (Conv3D)          (None, 6, 8, 8, 512)      3539456   \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 6, 8, 8, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 3, 4, 4, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 3, 2, 2, 512)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              25169920  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,705,858\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 71,704,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:10:10.118396Z",
     "start_time": "2024-11-14T03:08:58.150942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_tiktok = classification_model_tiktok.fit(\n",
    "    tiktok_train_generator,\n",
    "    validation_data=tiktok_val_generator,\n",
    "    epochs=num_epochs,\n",
    ")"
   ],
   "id": "a61ead3100b5d05e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.6941 - sparse_categorical_accuracy: 0.3846 - val_loss: 0.6926 - val_sparse_categorical_accuracy: 0.5833\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.6935 - sparse_categorical_accuracy: 0.4519 - val_loss: 0.6933 - val_sparse_categorical_accuracy: 0.4167\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.6927 - sparse_categorical_accuracy: 0.5769 - val_loss: 0.6933 - val_sparse_categorical_accuracy: 0.4167\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:10:20.248Z",
     "start_time": "2024-11-14T03:10:10.135433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Evaluate on TikTok test set\n",
    "base_c3d_results_tiktok = classification_model_tiktok.evaluate(tiktok_test_generator)\n",
    "print(\n",
    "    f\"TikTok Dataset - Test Loss: {base_c3d_results_tiktok[0]}, Test Accuracy: {base_c3d_results_tiktok[1]}\"\n",
    ")\n"
   ],
   "id": "7b2014bcbf1669c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 1s/step - loss: 0.6931 - sparse_categorical_accuracy: 0.4643\n",
      "TikTok Dataset - Test Loss: 0.693145751953125, Test Accuracy: 0.4642857015132904\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MSupCL implementation",
   "id": "dcd22c96fe0ecab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:10:20.279452Z",
     "start_time": "2024-11-14T03:10:20.264412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine training data from both datasets\n",
    "combined_train_videos = violence_train_videos + tiktok_train_videos\n",
    "combined_train_labels = violence_train_labels_np.tolist() + tiktok_train_labels_np.tolist()\n",
    "\n",
    "\n",
    "# Create a combined data generator\n",
    "combined_train_generator = VideoDataGenerator(\n",
    "    combined_train_videos,\n",
    "    combined_train_labels,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=True,\n",
    ")\n",
    "\n",
    "violence_train_generator_no_aug = VideoDataGenerator(\n",
    "    violence_train_videos,\n",
    "    violence_train_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=False,\n",
    ")\n",
    "\n",
    "tiktok_train_generator_no_aug = VideoDataGenerator(\n",
    "    tiktok_train_videos,\n",
    "    tiktok_train_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=False,\n",
    ")\n",
    "\n",
    "paired_train_generator = PairedDataGenerator(\n",
    "    violence_train_generator_no_aug,\n",
    "    tiktok_train_generator_no_aug,\n",
    "    batch_size=16,\n",
    ")"
   ],
   "id": "bddde4c8d28478eb",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:10:20.416619Z",
     "start_time": "2024-11-14T03:10:20.295598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msupcl_model = load_c3d_model(input_shape=input_shape, feature_dim=feature_dim)\n",
    "print(\"MSupCL Model Summary:\")\n",
    "msupcl_model.summary()"
   ],
   "id": "819725c0d757e7b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSupCL Model Summary:\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 12, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 12, 64, 64, 64)    5248      \n",
      "                                                                 \n",
      " max_pooling3d_10 (MaxPoolin  (None, 12, 32, 32, 64)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 12, 32, 32, 128)   221312    \n",
      "                                                                 \n",
      " max_pooling3d_11 (MaxPoolin  (None, 6, 16, 16, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 6, 16, 16, 256)    884992    \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 6, 16, 16, 256)    1769728   \n",
      "                                                                 \n",
      " max_pooling3d_12 (MaxPoolin  (None, 6, 8, 8, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_20 (Conv3D)          (None, 6, 8, 8, 512)      3539456   \n",
      "                                                                 \n",
      " conv3d_21 (Conv3D)          (None, 6, 8, 8, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_13 (MaxPoolin  (None, 3, 4, 4, 512)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_22 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " conv3d_23 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_14 (MaxPoolin  (None, 3, 2, 2, 512)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 4096)              25169920  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               2097664   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,704,832\n",
      "Trainable params: 71,704,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:11:52.399444Z",
     "start_time": "2024-11-14T03:10:20.443140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Train the model\n",
    "train_msupcl_model(msupcl_model, paired_train_generator, epochs=num_epochs, temperature=temperature)\n",
    "\n",
    "\n"
   ],
   "id": "73efa82bee2c74bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training Loss: 0.6147\n",
      "Epoch 2/3\n",
      "Training Loss: 0.6121\n",
      "Epoch 3/3\n",
      "Training Loss: 0.6110\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:18.593722Z",
     "start_time": "2024-11-14T03:11:52.423015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msupcl_c3d_result_violence, msupcl_c3d_result_tiktok = linear_evaluation(\n",
    "    msupcl_model,\n",
    "    combined_train_generator,\n",
    "    violence_test_generator,\n",
    "    tiktok_test_generator,\n",
    "    num_classes=num_classes,\n",
    ")\n"
   ],
   "id": "e437717914dc10fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27/27 [==============================] - 27s 977ms/step - loss: 2.6310 - accuracy: 0.4815\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 26s 973ms/step - loss: 1.8185 - accuracy: 0.4676\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 1.6371 - accuracy: 0.5093\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 26s 968ms/step - loss: 1.9938 - accuracy: 0.4491\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 1.5799 - accuracy: 0.5000\n",
      "Evaluating on Violence Test Set:\n",
      "7/7 [==============================] - 4s 494ms/step - loss: 0.6824 - accuracy: 0.5357\n",
      "Violence Test Loss: 0.6823827624320984, Test Accuracy: 0.5357142686843872\n",
      "Evaluating on TikTok Test Set:\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.7261 - accuracy: 0.4643\n",
      "TikTok Test Loss: 0.7260947227478027, Test Accuracy: 0.4642857015132904\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SSCL\n",
   "id": "d3c9e63ab1d19804"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:18.639449Z",
     "start_time": "2024-11-14T03:14:18.620781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "violence_train_sscl_generator = SSCLVideoDataGenerator(\n",
    "    violence_train_videos,\n",
    "    violence_train_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=True,\n",
    ")\n",
    "\n",
    "violence_val_sscl_generator = SSCLVideoDataGenerator(\n",
    "    violence_val_videos,\n",
    "    violence_val_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    double_view=False,\n",
    ")\n",
    "violence_test_sscl_generator = SSCLVideoDataGenerator(\n",
    "    violence_test_videos,\n",
    "    violence_test_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    double_view=False,\n",
    ")\n",
    "tiktok_train_sscl_generator = SSCLVideoDataGenerator(\n",
    "    tiktok_train_videos,\n",
    "    tiktok_train_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    augment=True,\n",
    ")\n",
    "tiktok_val_sscl_generator = SSCLVideoDataGenerator(\n",
    "    tiktok_val_videos,\n",
    "    tiktok_val_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    double_view=False,\n",
    ")\n",
    "tiktok_test_sscl_generator = SSCLVideoDataGenerator(\n",
    "    tiktok_test_videos,\n",
    "    tiktok_test_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    double_view=False,\n",
    ")\n",
    "\n"
   ],
   "id": "102fd8624d03d1ad",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:18.794021Z",
     "start_time": "2024-11-14T03:14:18.653242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sscl_model = load_c3d_sscl_model(input_shape=input_shape, feature_dim=feature_dim)\n",
    "print(\"SSCL Model Summary:\")\n",
    "sscl_model.summary()"
   ],
   "id": "aee466c25b3f1cd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSCL Model Summary:\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 12, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " conv3d_24 (Conv3D)          (None, 12, 64, 64, 64)    5248      \n",
      "                                                                 \n",
      " max_pooling3d_15 (MaxPoolin  (None, 12, 32, 32, 64)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_25 (Conv3D)          (None, 12, 32, 32, 128)   221312    \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPoolin  (None, 6, 16, 16, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_26 (Conv3D)          (None, 6, 16, 16, 256)    884992    \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 6, 16, 16, 256)    1769728   \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 6, 8, 8, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_28 (Conv3D)          (None, 6, 8, 8, 512)      3539456   \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 6, 8, 8, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 4, 4, 512)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 3, 4, 4, 512)      7078400   \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 3, 2, 2, 512)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6144)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4096)              25169920  \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " features (Dense)            (None, 512)               2097664   \n",
      "                                                                 \n",
      " projections (Dense)         (None, 512)               262656    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,967,488\n",
      "Trainable params: 71,967,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.716231Z",
     "start_time": "2024-11-14T03:14:18.823885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练SSCL模型（在暴力数据集上）\n",
    "train_simclr_model(sscl_model, violence_train_sscl_generator, epochs=num_epochs, temperature=temperature)\n"
   ],
   "id": "5e0cc1c5a6983fe1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"model_8\" is incompatible with the layer: expected shape=(None, 12, 64, 64, 3), found shape=(4, 16, 112, 112, 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 训练SSCL模型（在暴力数据集上）\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain_simclr_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43msscl_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mviolence_train_sscl_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\study\\CSi\\2024fall\\5341\\project\\Autism-Diagnosis-Transfer-Learning\\model_train.py:253\u001B[0m, in \u001B[0;36mtrain_simclr_model\u001B[1;34m(model, train_generator, epochs, temperature)\u001B[0m\n\u001B[0;32m    250\u001B[0m (x_i, x_j), _ \u001B[38;5;241m=\u001B[39m train_generator[step]\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[0;32m    252\u001B[0m     \u001B[38;5;66;03m# 获取投影后的特征\u001B[39;00m\n\u001B[1;32m--> 253\u001B[0m     _, z_i \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m     _, z_j \u001B[38;5;241m=\u001B[39m model(x_j, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;66;03m# 计算 NT-Xent 损失\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CSI5341\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CSI5341\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[1;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[1;32m--> 295\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    296\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    297\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincompatible with the layer: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    298\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    299\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdisplay_shape(x\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    300\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: Input 0 of layer \"model_8\" is incompatible with the layer: expected shape=(None, 12, 64, 64, 3), found shape=(4, 16, 112, 112, 3)"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 在线性评估中使用训练好的SSCL模型（暴力数据集）\n",
    "sscl_c3d_result_violence = linear_evaluation_sscl(\n",
    "    sscl_model,\n",
    "    violence_train_sscl_generator,\n",
    "    violence_val_sscl_generator,\n",
    "    violence_test_sscl_generator,\n",
    "    num_classes=num_classes,\n",
    ")"
   ],
   "id": "e58ef713eda7b3cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_simclr_model(sscl_model, tiktok_train_sscl_generator, epochs=num_epochs, temperature=temperature)\n",
   "id": "514804957bffeea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sscl_c3d_result_tiktok = linear_evaluation_sscl(\n",
    "    sscl_model,\n",
    "    tiktok_train_sscl_generator,\n",
    "    tiktok_val_sscl_generator,\n",
    "    tiktok_test_sscl_generator,\n",
    "    num_classes=num_classes,\n",
    ")"
   ],
   "id": "ddaf4a0182b84024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## R2+1d_18 model",
   "id": "9cb372ba1eb23a27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:11:39.129850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "id": "cffad71d1c0257dc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:11:39.161492Z"
    }
   },
   "cell_type": "code",
   "source": "base_model = load_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim, include_top=False)",
   "id": "a162dca1b9a0d925",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:11:39.665061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "features = base_model.output\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(features)\n",
    "classification_model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "classification_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ],
   "id": "f9d1d7a415cdd8cb",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:11:39.711871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train on Violence dataset\n",
    "history_violence = classification_model.fit(\n",
    "    violence_train_generator,\n",
    "    validation_data=violence_val_generator,\n",
    "    epochs=num_epochs\n",
    ")"
   ],
   "id": "859b3c091244e0da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 20s 744ms/step - loss: 0.6933 - sparse_categorical_accuracy: 0.4907 - val_loss: 0.6933 - val_sparse_categorical_accuracy: 0.4286\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 19s 691ms/step - loss: 0.6931 - sparse_categorical_accuracy: 0.4954 - val_loss: 0.6929 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 19s 679ms/step - loss: 0.6936 - sparse_categorical_accuracy: 0.4907 - val_loss: 0.6928 - val_sparse_categorical_accuracy: 0.6429\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 19s 687ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.5185 - val_loss: 0.6928 - val_sparse_categorical_accuracy: 0.5357\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 19s 684ms/step - loss: 0.6937 - sparse_categorical_accuracy: 0.5046 - val_loss: 0.6927 - val_sparse_categorical_accuracy: 0.5357\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 19s 698ms/step - loss: 0.6942 - sparse_categorical_accuracy: 0.4352 - val_loss: 0.6927 - val_sparse_categorical_accuracy: 0.5357\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 19s 708ms/step - loss: 0.6934 - sparse_categorical_accuracy: 0.5000 - val_loss: 0.6924 - val_sparse_categorical_accuracy: 0.5357\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 19s 717ms/step - loss: 0.6939 - sparse_categorical_accuracy: 0.5000 - val_loss: 0.6926 - val_sparse_categorical_accuracy: 0.5357\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 20s 737ms/step - loss: 0.6938 - sparse_categorical_accuracy: 0.4954 - val_loss: 0.6927 - val_sparse_categorical_accuracy: 0.5714\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 19s 699ms/step - loss: 0.6928 - sparse_categorical_accuracy: 0.5278 - val_loss: 0.6924 - val_sparse_categorical_accuracy: 0.5357\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:14:53.090535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on Violence test set\n",
    "base_r2plus1d_results_violence = classification_model.evaluate(violence_test_generator)\n",
    "print(f\"Violence Dataset - Test Loss: {base_r2plus1d_results_violence[0]}, Test Accuracy: {base_r2plus1d_results_violence[1]}\")"
   ],
   "id": "fc9df2f8b212dafb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 8s 556ms/step - loss: 0.6932 - sparse_categorical_accuracy: 0.4833\n",
      "Violence Dataset - Test Loss: 0.6932312250137329, Test Accuracy: 0.4833333194255829\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:15:01.628509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_tiktok = classification_model.fit(\n",
    "    tiktok_train_generator,\n",
    "    validation_data=tiktok_val_generator,\n",
    "    epochs=num_epochs\n",
    ")"
   ],
   "id": "211377d3d087cbb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 60s 2s/step - loss: 0.6909 - sparse_categorical_accuracy: 0.5463 - val_loss: 0.6960 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6892 - sparse_categorical_accuracy: 0.5556 - val_loss: 0.7012 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 59s 2s/step - loss: 0.6866 - sparse_categorical_accuracy: 0.5556 - val_loss: 0.7015 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6843 - sparse_categorical_accuracy: 0.5648 - val_loss: 0.7050 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6828 - sparse_categorical_accuracy: 0.5648 - val_loss: 0.7045 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6804 - sparse_categorical_accuracy: 0.5648 - val_loss: 0.7068 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6847 - sparse_categorical_accuracy: 0.5463 - val_loss: 0.7068 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6837 - sparse_categorical_accuracy: 0.5463 - val_loss: 0.7073 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 59s 2s/step - loss: 0.6827 - sparse_categorical_accuracy: 0.5463 - val_loss: 0.7083 - val_sparse_categorical_accuracy: 0.3929\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 58s 2s/step - loss: 0.6830 - sparse_categorical_accuracy: 0.5463 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.3929\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:24:47.894372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate on TikTok test set\n",
    "base_r2plus1d_results_tiktok = classification_model.evaluate(tiktok_test_generator)\n",
    "print(f\"TikTok Dataset - Test Loss: {base_r2plus1d_results_tiktok[0]}, Test Accuracy: {base_r2plus1d_results_tiktok[1]}\")"
   ],
   "id": "51f74b0136087fa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 26s 2s/step - loss: 0.6990 - sparse_categorical_accuracy: 0.4500\n",
      "TikTok Dataset - Test Loss: 0.6989744901657104, Test Accuracy: 0.44999998807907104\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:25:15.934698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "msupcl_model = load_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim, include_top=False)"
   ],
   "id": "79b199cbcef78608",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:25:16.407745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msupcl_c3d_result_violence, msupcl_c3d_result_tiktok = linear_evaluation(\n",
    "    msupcl_model,\n",
    "    combined_train_generator,\n",
    "    violence_test_generator,\n",
    "    tiktok_test_generator,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "violence_train_generator_no_aug = VideoDataGenerator(violence_train_videos, violence_train_labels_np, batch_size=batch_size, shuffle=True, augment=False)\n",
    "tiktok_train_generator_no_aug = VideoDataGenerator(tiktok_train_videos, tiktok_train_labels_np, batch_size=batch_size, shuffle=True, augment=False)\n",
    "paired_train_generator = PairedDataGenerator(violence_train_generator_no_aug, tiktok_train_generator_no_aug)"
   ],
   "id": "d6ca19638578676a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:25:16.439461Z"
    }
   },
   "cell_type": "code",
   "source": "train_msupcl_model(msupcl_model, paired_train_generator, epochs=num_epochs)",
   "id": "e99577651e50c31e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.6702\n",
      "Epoch 2/10\n",
      "Training Loss: 0.6660\n",
      "Epoch 3/10\n",
      "Training Loss: 0.6695\n",
      "Epoch 4/10\n",
      "Training Loss: 0.6684\n",
      "Epoch 5/10\n",
      "Training Loss: 0.6684\n",
      "Epoch 6/10\n",
      "Training Loss: 0.6688\n",
      "Epoch 7/10\n",
      "Training Loss: 0.6676\n",
      "Epoch 8/10\n",
      "Training Loss: 0.6655\n",
      "Epoch 9/10\n",
      "Training Loss: 0.6670\n",
      "Epoch 10/10\n",
      "Training Loss: 0.6648\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:38:15.274183Z"
    }
   },
   "cell_type": "code",
   "source": "msupcl_r2plus1d_result_violence, msupcl_r2plus1d_result_tiktok = linear_evaluation(msupcl_model, combined_train_generator, violence_test_generator,tiktok_test_generator, num_classes=num_classes)",
   "id": "55d925dd4feb4f2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55/55 [==============================] - 74s 1s/step - loss: 0.8126 - accuracy: 0.5341 - val_loss: 0.7668 - val_accuracy: 0.4500\n",
      "Epoch 2/5\n",
      "55/55 [==============================] - 72s 1s/step - loss: 0.7003 - accuracy: 0.5341 - val_loss: 0.7598 - val_accuracy: 0.5833\n",
      "Epoch 3/5\n",
      "55/55 [==============================] - 72s 1s/step - loss: 0.6988 - accuracy: 0.5705 - val_loss: 0.7940 - val_accuracy: 0.4500\n",
      "Epoch 4/5\n",
      "55/55 [==============================] - 72s 1s/step - loss: 0.7092 - accuracy: 0.5477 - val_loss: 0.7422 - val_accuracy: 0.4667\n",
      "Epoch 5/5\n",
      "55/55 [==============================] - 72s 1s/step - loss: 0.7077 - accuracy: 0.5636 - val_loss: 0.7156 - val_accuracy: 0.4833\n",
      "Evaluating on Violence Test Set:\n",
      "15/15 [==============================] - 8s 553ms/step - loss: 0.7526 - accuracy: 0.4500\n",
      "Violence Test Loss: 0.7525920867919922, Test Accuracy: 0.44999998807907104\n",
      "Evaluating on TikTok Test Set:\n",
      "15/15 [==============================] - 26s 2s/step - loss: 0.7540 - accuracy: 0.4667\n",
      "TikTok Test Loss: 0.7539868950843811, Test Accuracy: 0.46666666865348816\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SSCL ",
   "id": "4f7bfa020db1c968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-14T00:35:36.330264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "violence_train_sscl_generator = SSCLVideoDataGenerator(\n",
    "    violence_train_videos,\n",
    "    violence_train_labels_np,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    augment=True\n",
    ")\n",
    "\n",
    "violence_val_single_sscl_generator = SSCLVideoDataGenerator(\n",
    "    violence_val_videos,\n",
    "    violence_val_labels_np,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    augment=True, \n",
    "    double_view=False\n",
    ")\n",
    "\n",
    "\n",
    "violence_test_sscl_generator = SSCLVideoDataGenerator(\n",
    "    violence_test_videos,\n",
    "    violence_test_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False, \n",
    "    double_view=False\n",
    ")\n",
    "\n",
    "tiktok_train_sscl_generator = SSCLVideoDataGenerator(\n",
    "    tiktok_train_videos,\n",
    "    tiktok_train_labels_np,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    augment=True\n",
    ")\n",
    "\n",
    "tiktok_val_single_sscl_generator = SSCLVideoDataGenerator(\n",
    "    tiktok_val_videos,\n",
    "    tiktok_val_labels_np,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    augment=True, \n",
    "    double_view=False\n",
    ")\n",
    "\n",
    "tiktok_test_sscl_generator = SSCLVideoDataGenerator(\n",
    "    tiktok_test_videos,\n",
    "    tiktok_test_labels_np,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False, \n",
    "    double_view=False\n",
    ")\n",
    "\n",
    "violence_train_single_sscl_generator = SSCLVideoDataGenerator(violence_train_videos,violence_train_labels_np,batch_size=batch_size, shuffle=True, augment=True, double_view=False)\n",
    "\n",
    "\n",
    "tiktok_train_single_sscl_generator = SSCLVideoDataGenerator(tiktok_train_videos,tiktok_train_labels_np,batch_size=batch_size, shuffle=True, augment=True, double_view=False)\n"
   ],
   "id": "514b2d7164eb17de",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T22:44:53.709506Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1, Loss: 1.9161\n",
      "Epoch 2/10\n",
      "Epoch 2, Loss: 1.9715\n",
      "Epoch 3/10\n",
      "Epoch 3, Loss: 1.9399\n",
      "Epoch 4/10\n",
      "Epoch 4, Loss: 1.9738\n",
      "Epoch 5/10\n",
      "Epoch 5, Loss: 1.9459\n",
      "Epoch 6/10\n",
      "Epoch 6, Loss: 1.9459\n",
      "Epoch 7/10\n",
      "Epoch 7, Loss: 1.9459\n",
      "Epoch 8/10\n",
      "Epoch 8, Loss: 1.9459\n",
      "Epoch 9/10\n",
      "Epoch 9, Loss: 1.9459\n",
      "Epoch 10/10\n",
      "Epoch 10, Loss: 1.9459\n"
     ]
    }
   ],
   "execution_count": 31,
   "source": [
    "\n",
    "\n",
    "model = load_c3d_sscl_model(input_shape=input_shape, feature_dim=feature_dim)\n",
    "\n",
    "train_simclr_model(model, violence_train_sscl_generator, epochs=num_epochs)"
   ],
   "id": "dbf378a3928b4652"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T23:00:18.926324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "sscl_c3d_result_violence = linear_evaluation_sscl(model, violence_train_single_sscl_generator, violence_val_single_sscl_generator,violence_test_sscl_generator, num_classes=num_classes)"
   ],
   "id": "1099a8f1c735b285",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.6932 - accuracy: 0.4907 - val_loss: 0.6932 - val_accuracy: 0.4643\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 61s 2s/step - loss: 0.6932 - accuracy: 0.4815 - val_loss: 0.6931 - val_accuracy: 0.5357\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 64s 2s/step - loss: 0.6932 - accuracy: 0.4630 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 62s 2s/step - loss: 0.6932 - accuracy: 0.4815 - val_loss: 0.6933 - val_accuracy: 0.4643\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 62s 2s/step - loss: 0.6932 - accuracy: 0.5278 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "15/15 [==============================] - 27s 2s/step - loss: 0.6932 - accuracy: 0.4833\n",
      "Test Loss: 0.6931995749473572, Test Accuracy: 0.4833333194255829\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.722238Z",
     "start_time": "2024-11-13T23:06:03.971236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "train_simclr_model(model, tiktok_train_sscl_generator, epochs=num_epochs, temperature=temperature)\n",
    "sscl_c3d_result_tiktok = linear_evaluation_sscl(model, tiktok_train_single_sscl_generator,tiktok_val_single_sscl_generator,tiktok_test_sscl_generator, num_classes=num_classes)"
   ],
   "id": "68bd9cce69c4a136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1, Loss: 1.9459\n",
      "Epoch 2/10\n",
      "Epoch 2, Loss: 1.9459\n",
      "Epoch 3/10\n",
      "Epoch 3, Loss: 1.9459\n",
      "Epoch 4/10\n",
      "Epoch 4, Loss: 1.9459\n",
      "Epoch 5/10\n",
      "Epoch 5, Loss: 1.9459\n",
      "Epoch 6/10\n",
      "Epoch 6, Loss: 1.9459\n",
      "Epoch 7/10\n",
      "Epoch 7, Loss: 1.9459\n",
      "Epoch 8/10\n",
      "Epoch 8, Loss: 1.9459\n",
      "Epoch 9/10\n",
      "Epoch 9, Loss: 1.9459\n",
      "Epoch 10/10\n",
      "Epoch 10, Loss: 1.9459\n",
      "Epoch 1/5\n",
      "27/27 [==============================] - 183s 7s/step - loss: 0.6930 - accuracy: 0.6019 - val_loss: 0.6933 - val_accuracy: 0.4286\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 181s 7s/step - loss: 0.6931 - accuracy: 0.5648 - val_loss: 0.6937 - val_accuracy: 0.3929\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 180s 7s/step - loss: 0.6929 - accuracy: 0.5556 - val_loss: 0.6936 - val_accuracy: 0.4286\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 179s 7s/step - loss: 0.6928 - accuracy: 0.5463 - val_loss: 0.6939 - val_accuracy: 0.3929\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 180s 7s/step - loss: 0.6928 - accuracy: 0.5463 - val_loss: 0.6937 - val_accuracy: 0.4286\n",
      "15/15 [==============================] - 81s 5s/step - loss: 0.6936 - accuracy: 0.4500\n",
      "Test Loss: 0.6935514807701111, Test Accuracy: 0.44999998807907104\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.725258900Z",
     "start_time": "2024-11-14T00:36:36.414606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = load_sscl_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim)\n",
    "train_simclr_model(model, violence_train_sscl_generator, epochs=num_epochs, temperature=temperature)"
   ],
   "id": "12a8dfd783779f90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1, Loss: 1.5959\n",
      "Epoch 2/10\n",
      "Epoch 2, Loss: 1.5192\n",
      "Epoch 3/10\n",
      "Epoch 3, Loss: 1.4695\n",
      "Epoch 4/10\n",
      "Epoch 4, Loss: 1.2889\n",
      "Epoch 5/10\n",
      "Epoch 5, Loss: 1.2568\n",
      "Epoch 6/10\n",
      "Epoch 6, Loss: 1.4238\n",
      "Epoch 7/10\n",
      "Epoch 7, Loss: 1.2884\n",
      "Epoch 8/10\n",
      "Epoch 8, Loss: 1.2220\n",
      "Epoch 9/10\n",
      "Epoch 9, Loss: 1.2543\n",
      "Epoch 10/10\n",
      "Epoch 10, Loss: 1.1313\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.725258900Z",
     "start_time": "2024-11-14T00:20:20.273743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "sscl_r2plus1d_result_violence = linear_evaluation_sscl(model, violence_train_single_sscl_generator, violence_val_single_sscl_generator,violence_test_sscl_generator, num_classes=num_classes)"
   ],
   "id": "7f0843e9baeb0697",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27/27 [==============================] - 67s 3s/step - loss: nan - accuracy: 0.5093 - val_loss: nan - val_accuracy: 0.5357\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 64s 2s/step - loss: nan - accuracy: 0.5093 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 61s 2s/step - loss: nan - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5357\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 62s 2s/step - loss: nan - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5357\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 63s 2s/step - loss: nan - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5000\n",
      "15/15 [==============================] - 28s 2s/step - loss: nan - accuracy: 0.4833\n",
      "Test Loss: nan, Test Accuracy: 0.4833333194255829\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:14:20.725258900Z",
     "start_time": "2024-11-14T00:26:08.472127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_simclr_model(model, tiktok_train_sscl_generator, epochs=num_epochs, temperature=temperature)\n",
    "sscl_r2plus1d_result_tiktok = linear_evaluation_sscl(model, tiktok_train_single_sscl_generator,tiktok_val_single_sscl_generator,tiktok_test_sscl_generator, num_classes=num_classes)"
   ],
   "id": "c9d9fa21b07181d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_simclr_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtiktok_train_sscl_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m sscl_r2plus1d_result_tiktok \u001B[38;5;241m=\u001B[39m linear_evaluation_sscl(model, tiktok_train_single_sscl_generator,tiktok_val_single_sscl_generator,tiktok_test_sscl_generator, num_classes\u001B[38;5;241m=\u001B[39mnum_classes)\n",
      "File \u001B[1;32mF:\\study\\CSi\\2024fall\\5341\\project\\Autism-Diagnosis-Transfer-Learning\\model_train.py:256\u001B[0m, in \u001B[0;36mtrain_simclr_model\u001B[1;34m(model, train_generator, epochs, temperature)\u001B[0m\n\u001B[0;32m    253\u001B[0m epoch_loss_avg \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mMean()\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(train_generator)):\n\u001B[1;32m--> 256\u001B[0m     (x_i, x_j), _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_generator\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    258\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[0;32m    260\u001B[0m         z_i \u001B[38;5;241m=\u001B[39m model(x_i, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mF:\\study\\CSi\\2024fall\\5341\\project\\Autism-Diagnosis-Transfer-Learning\\data_uniform_sscl.py:56\u001B[0m, in \u001B[0;36mSSCLVideoDataGenerator.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     54\u001B[0m frames_aug2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdouble_view:\n\u001B[1;32m---> 56\u001B[0m     frames_aug2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_frames\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# 预处理帧\u001B[39;00m\n\u001B[0;32m     59\u001B[0m frames_aug1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_frames(frames_aug1)\n",
      "File \u001B[1;32mF:\\study\\CSi\\2024fall\\5341\\project\\Autism-Diagnosis-Transfer-Learning\\data_uniform_sscl.py:113\u001B[0m, in \u001B[0;36mSSCLVideoDataGenerator.augment_frames\u001B[1;34m(self, frames)\u001B[0m\n\u001B[0;32m    111\u001B[0m     frame \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mflip(frame, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# 颜色抖动\u001B[39;00m\n\u001B[1;32m--> 113\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolor_jitter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;66;03m# 高斯模糊\u001B[39;00m\n\u001B[0;32m    115\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgaussian_blur(frame)\n",
      "File \u001B[1;32mF:\\study\\CSi\\2024fall\\5341\\project\\Autism-Diagnosis-Transfer-Learning\\data_uniform_sscl.py:232\u001B[0m, in \u001B[0;36mSSCLVideoDataGenerator.color_jitter\u001B[1;34m(self, frame)\u001B[0m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.8\u001B[39m:\n\u001B[0;32m    231\u001B[0m     hue_factor \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m--> 232\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mHSV\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    233\u001B[0m     img[:, :, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m (img[:, :, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mint\u001B[39m(hue_factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m)) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m255\u001B[39m\n\u001B[0;32m    234\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHSV\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CSI5341\\lib\\site-packages\\PIL\\Image.py:1152\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m   1149\u001B[0m     dither \u001B[38;5;241m=\u001B[39m Dither\u001B[38;5;241m.\u001B[39mFLOYDSTEINBERG\n\u001B[0;32m   1151\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1152\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdither\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1153\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m   1154\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1155\u001B[0m         \u001B[38;5;66;03m# normalize source image and try again\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
