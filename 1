你好，我项目是探索论文中提到的方式MSupCL是否对视频harmful content classification的效果有所改善，baseline采取了标准的BinClass使用binary cross-entropy loss进行训练。还需要实现SSCL对比训练方法。使用的数据库有两个，其一为Violence Video，即视频中是否有暴力内容被分为了两类。另一个为tiktok视频数据集，其中被分为harmful content和safe。

请严格参考论文“Activity-based Early Autism Diagnosis Using
A Multi-Dataset Supervised Contrastive Learning Approach”
其中loss公式的定义为：
L_{MSupCL}^{a} = -\frac{1}{|P_a|} \sum_{v_p \in P_a} \log \frac{\exp(z_a \cdot z_p / \tau)}{\sum_{v_k \in K_a} \exp(z_a \cdot z_k / \tau)}


检查我代码是否完全符合论文中描述的方法，因为我的代码的结果为Baseline，MSUPCL和SSCL的linear evaluation的结果一模一样，这表示两种方法的并没有对linear evaluation产生任何影响，这是不合理的。请所有代码的正确性,并检查他们是否符合原文。如果需要你可以删除，修改，添加仍和代码。如果需要你可以大规模修改代码，甚至重新开始。


下列为代码：

model_train.py:
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses, metrics


def load_c3d_model(input_shape=(12, 64, 64, 3), feature_dim=512):
    # 定义输入
    inputs = layers.Input(shape=input_shape)

    # 第一层卷积和池化
    x = layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same')(inputs)
    x = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))(x)

    # 第二层卷积和池化
    x = layers.Conv3D(128, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)

    # 第三、四层卷积和池化
    x = layers.Conv3D(256, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.Conv3D(256, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))(x)

    # 第五、六层卷积和池化
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)

    # 第七、八层卷积和池化
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))(x)

    # 展平和全连接层
    x = layers.Flatten()(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)

    # 输出特征向量
    features = layers.Dense(feature_dim, activation='relu')(x)

    # 构建模型
    model = models.Model(inputs=inputs, outputs=features)

    return model


def supervised_contrastive_loss(labels, features, temperature=0.07):
    # 特征归一化
    features = tf.math.l2_normalize(features, axis=1)
    batch_size = tf.shape(features)[0]

    # 计算相似度矩阵
    similarity_matrix = tf.matmul(features, features, transpose_b=True)  # (batch_size, batch_size)

    # 获取标签相等的矩阵
    labels = tf.reshape(labels, (batch_size, 1))
    labels_equal = tf.equal(labels, tf.transpose(labels))  # (batch_size, batch_size)
    labels_equal = tf.cast(labels_equal, tf.float32)

    # 掩码，排除自身
    mask = tf.cast(tf.eye(batch_size), tf.bool)
    labels_equal = tf.where(mask, tf.zeros_like(labels_equal), labels_equal)
    similarity_matrix = tf.where(mask, tf.zeros_like(similarity_matrix), similarity_matrix)

    # 计算分子和分母
    exp_similarity = tf.exp(similarity_matrix / temperature)
    sum_exp = tf.reduce_sum(exp_similarity, axis=1)
    pos_exp = tf.reduce_sum(exp_similarity * labels_equal, axis=1)

    # 计算损失
    loss = -tf.math.log(pos_exp / sum_exp)
    loss = tf.reduce_mean(loss)
    return loss


def train_msupcl_model(model, paired_train_generator, epochs=10, temperature=0.07):
    optimizer = optimizers.Adam(learning_rate=1e-4)
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        epoch_loss_avg = tf.keras.metrics.Mean()
        for step in range(len(paired_train_generator)):
            (X1_batch, X2_batch), y_batch = paired_train_generator[step]
            with tf.GradientTape() as tape:
                features1 = model(X1_batch, training=True)
                features2 = model(X2_batch, training=True)
                # 将两个特征拼接
                features = tf.concat([features1, features2], axis=0)
                # 标签也拼接
                labels = tf.concat([y_batch, y_batch], axis=0)
                # 计算损失
                loss = supervised_contrastive_loss(labels, features, temperature)
            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            epoch_loss_avg.update_state(loss)
        print(f"Training Loss: {epoch_loss_avg.result():.4f}")


def linear_evaluation(model, train_generator,test_generator1,test_generator2, num_classes=2):
    # Freeze the base model
    for layer in model.layers:
        layer.trainable = False

    # Add a classification head
    features = model.output
    outputs = layers.Dense(num_classes, activation='softmax')(features)
    classifier_model = models.Model(inputs=model.input, outputs=outputs)

    # Compile the classifier
    classifier_model.compile(
        loss=losses.SparseCategoricalCrossentropy(),
        optimizer=optimizers.Adam(learning_rate=1e-4),
        metrics=['accuracy']
    )

    # Train the classifier on the combined dataset
    classifier_model.fit(
        train_generator,
        epochs=5
    )

    # Evaluate on test sets
    print("Evaluating on Violence Test Set:")
    results_violence = classifier_model.evaluate(test_generator1)
    print(f"Violence Test Loss: {results_violence[0]}, Test Accuracy: {results_violence[1]}")

    print("Evaluating on TikTok Test Set:")
    results_tiktok = classifier_model.evaluate(test_generator2)
    print(f"TikTok Test Loss: {results_tiktok[0]}, Test Accuracy: {results_tiktok[1]}")

    return results_violence, results_tiktok


def nt_xent_loss(z_i, z_j, temperature=0.5):
    batch_size = tf.shape(z_i)[0]

    # 归一化特征向量
    z_i = tf.math.l2_normalize(z_i, axis=1)
    z_j = tf.math.l2_normalize(z_j, axis=1)

    # 拼接特征向量
    z = tf.concat([z_i, z_j], axis=0)  # (2*batch_size, feature_dim)

    # 计算相似度矩阵
    similarity_matrix = tf.matmul(z, z, transpose_b=True)  # (2*batch_size, 2*batch_size)
    similarity_matrix = similarity_matrix / temperature

    # 创建掩码，排除自身对比
    mask = tf.eye(2 * batch_size)
    logits = similarity_matrix - mask * 1e9  # 将自身相似度设为一个极小值

    # 创建标签：对于每个样本，其正样本索引为 (i + batch_size) % (2 * batch_size)
    positive_indices = tf.concat([tf.range(batch_size, 2 * batch_size), tf.range(0, batch_size)], axis=0)
    labels = positive_indices

    # 计算交叉熵损失
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)
    loss = tf.reduce_mean(loss)
    return loss

def load_c3d_sscl_model(input_shape=(16, 112, 112, 3), feature_dim=512):
    # 定义输入
    inputs = layers.Input(shape=input_shape)

    # 第一层卷积和池化
    x = layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same')(inputs)
    x = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))(x)

    # 第二层卷积和池化
    x = layers.Conv3D(128, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)

    # 第三、四层卷积和池化
    x = layers.Conv3D(256, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.Conv3D(256, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))(x)

    # 第五、六层卷积和池化
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)

    # 第七、八层卷积和池化
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.Conv3D(512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2))(x)

    # 展平和全连接层
    x = layers.Flatten()(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.5)(x)

    # 输出特征向量
    features = layers.Dense(feature_dim, activation='relu', name='features')(x)

    projections = layers.Dense(feature_dim, name='projections')(features)

    # 构建模型
    model = models.Model(inputs=inputs, outputs=[features, projections])

    return model

def train_simclr_model(model, train_generator, epochs=10, temperature=0.5):
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        epoch_loss_avg = tf.keras.metrics.Mean()
        for step in range(len(train_generator)):
            (x_i, x_j), _ = train_generator[step]
            with tf.GradientTape() as tape:
                # 获取投影后的特征
                _, z_i = model(x_i, training=True)
                _, z_j = model(x_j, training=True)
                # 计算 NT-Xent 损失
                loss = nt_xent_loss(z_i, z_j, temperature)
            # 反向传播和优化
            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            epoch_loss_avg.update_state(loss)
        print(f"Epoch {epoch + 1}, Loss: {epoch_loss_avg.result():.4f}")

def linear_evaluation_sscl(model, train_generator, val_generator, test_generator, num_classes=2):
    # 冻结编码器参数
    for layer in model.layers:
        layer.trainable = False
    # 定义输入为模型的输入
    inputs = model.input
    # 使用features作为特征
    features = model.get_layer('features').output
    # 添加分类层
    outputs = layers.Dense(num_classes, activation='softmax')(features)
    classifier_model = models.Model(inputs=inputs, outputs=outputs)
    # 编译模型
    classifier_model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        metrics=['accuracy']
    )
    # 训练分类器
    classifier_model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=5
    )
    # 评估模型
    results = classifier_model.evaluate(test_generator)
    print(f"Test Loss: {results[0]}, Test Accuracy: {results[1]}")
    return results

    data_uniform_sup.py
    import os
import random
import math
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import Sequence

seed = 2042
np.random.seed(seed)
random.seed(seed)
tf.random.set_seed(seed)

CLIP_LEN = 12          # 每个视频剪辑的帧数
RESIZE_HEIGHT = 64    # 帧的调整高度
CROP_SIZE = 64        # 裁剪高度
size2 = 64            # 裁剪宽度


class VideoDataGenerator(Sequence):
    def __init__(self, dataset_paths, labels, batch_size=1, shuffle=True, split='train', augment=False):
        self.dataset_paths = dataset_paths  # 视频文件的路径列表
        self.labels = labels  # 对应的视频标签列表
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.split = split
        self.on_epoch_end()
        self.augment = augment

    def __len__(self):
        return int(np.floor(len(self.dataset_paths) / self.batch_size))

    def __getitem__(self, index):
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_paths = [self.dataset_paths[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]
        X, y = self.__data_generation(batch_paths, batch_labels)
        return X, y

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.dataset_paths))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_paths, batch_labels):
        X = []
        y = []
        for i, video_path in enumerate(batch_paths):
            frames = self.load_frames(video_path)
            if self.split == 'train' and self.augment:
                # 生成两个增强版本
                frames_1 = self.augment_frames(frames)
                frames_2 = self.augment_frames(frames)
                frames_1 = self.preprocess_frames(frames_1)
                frames_2 = self.preprocess_frames(frames_2)
                X.append(frames_1)
                y.append(batch_labels[i])
                X.append(frames_2)
                y.append(batch_labels[i])
            else:
                frames = self.preprocess_frames(frames)
                X.append(frames)
                y.append(batch_labels[i])
        X = np.array(X)
        y = np.array(y)

        return X, y

    def augment_frames(self, frames):
        if random.random() < 0.5:
            frames = frames[:, :, ::-1, :]
        return frames

    def load_frames(self, video_path):
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        if total_frames <= 0:
            # Handle cases where CAP_PROP_FRAME_COUNT is not available
            # Fallback to reading frames one by one (less efficient)
            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
            cap.release()
            frames = np.array(frames).astype(np.uint8)
            total_frames = len(frames)
            if total_frames < CLIP_LEN:
                # Repeat frames to reach CLIP_LEN
                repeat_times = (CLIP_LEN - total_frames) // total_frames + 1
                frames = np.tile(frames, (repeat_times, 1, 1, 1))[:CLIP_LEN]
            else:
                # Uniformly sample CLIP_LEN frames
                indices = np.linspace(0, total_frames - 1, CLIP_LEN).astype(int)
                frames = frames[indices]
        else:
            # Read only the required frames
            frames = []
            if total_frames < CLIP_LEN:
                # Read all frames and repeat to reach CLIP_LEN
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    frames.append(frame)
                frames = np.array(frames).astype(np.uint8)
                repeat_times = (CLIP_LEN - total_frames) // total_frames + 1
                frames = np.tile(frames, (repeat_times, 1, 1, 1))[:CLIP_LEN]
            else:
                # Calculate frame indices to sample
                indices = np.linspace(0, total_frames - 1, CLIP_LEN).astype(int)
                for idx in indices:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    if not ret:
                        break
                    frames.append(frame)
                frames = np.array(frames).astype(np.uint8)
        cap.release()
        return frames

    def preprocess_frames(self, frames):
        # 包含 resize、normalize、to_tensor 等操作
        frames = self.crop(frames, CLIP_LEN, CROP_SIZE, size2)
        frames = self.resize(frames)
        frames = self.normalize(frames)
        frames = self.to_tensor(frames)
        return frames


    def resize(self, frames):
        resized_frames = []
        for frame in frames:
            frame = cv2.resize(frame, (RESIZE_HEIGHT , RESIZE_HEIGHT ), interpolation=cv2.INTER_LINEAR)
            resized_frames.append(frame)
        return np.array(resized_frames)

    def normalize(self, frames):
        return frames.astype(np.float32) / 255.0

    def to_tensor(self, frames):
        return frames

    def random_flip(self, frames):
        if random.random() < 0.5:
            frames = frames[:, :, ::-1, :]
        return frames

    def crop(self, frames, clip_len, crop_size, crop_size2):
        if frames.shape[0] > clip_len:
            time_index = random.randint(0, frames.shape[0] - clip_len)
        else:
            time_index = 0
        height_index = random.randint(0, frames.shape[1] - crop_size)
        width_index = random.randint(0, frames.shape[2] - crop_size2)
        frames = frames[time_index:time_index + clip_len, height_index:height_index + crop_size,
                 width_index:width_index + crop_size2, :]
        if frames.shape[0] < clip_len:
            pad_num = clip_len - frames.shape[0]
            frames = np.concatenate((frames, frames[:pad_num]), axis=0)
        return frames

paired_generator.py:
import tensorflow as tf
import numpy as np


class PairedDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, generator1, generator2, batch_size=16):
        self.generator1 = generator1
        self.generator2 = generator2
        self.batch_size = batch_size
        self.on_epoch_end()

    def __len__(self):
        return min(len(self.generator1), len(self.generator2))

    def __getitem__(self, index):
        X1, y1 = self.generator1[index]
        X2, y2 = self.generator2[index]
        # 根据标签匹配正样本对，非匹配的为负样本对
        positive_pairs = []
        negative_pairs = []
        for i in range(len(y1)):
            for j in range(len(y2)):
                if y1[i] == y2[j]:
                    positive_pairs.append((X1[i], X2[j], y1[i]))
                else:
                    negative_pairs.append((X1[i], X2[j], y1[i]))
        # 组合正负样本对
        X1_batch = np.array([pair[0] for pair in positive_pairs + negative_pairs])
        X2_batch = np.array([pair[1] for pair in positive_pairs + negative_pairs])
        y_batch = np.array([pair[2] for pair in positive_pairs + negative_pairs])
        return (X1_batch, X2_batch), y_batch

    def on_epoch_end(self):
        pass

data_uniform_sscl.py:
import os
import random
import math
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image, ImageEnhance, ImageFilter
from tensorflow.keras.utils import Sequence

seed = 2042
np.random.seed(seed)
random.seed(seed)
tf.random.set_seed(seed)

CLIP_LEN = 16          # 每个视频剪辑的帧数
RESIZE_HEIGHT = 128    # 帧的调整高度
CROP_SIZE = 112        # 裁剪高度
size2 = 112            # 裁剪宽度


class SSCLVideoDataGenerator(Sequence):
    def __init__(self, dataset_paths, labels, batch_size=1, shuffle=True, split='train', augment=True,double_view=True):
        self.indexes = None
        self.dataset_paths = dataset_paths  # 视频文件的路径列表
        self.labels = labels  # 对应的视频标签列表
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.split = split
        self.on_epoch_end()
        self.augment = augment
        self.double_view = double_view


    def __len__(self):
        return int(np.floor(len(self.dataset_paths) / self.batch_size))

    def __getitem__(self, index):
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        video_paths_temp = [self.dataset_paths[k] for k in indexes]
        labels_temp = [self.labels[k] for k in indexes]

        X1_batch = []
        X2_batch = []
        y_batch = []

        for i in range(len(video_paths_temp)):
            video_path = video_paths_temp[i]
            label = labels_temp[i]

            frames = self.load_frames(video_path)

            # 生成增强视图
            frames_aug1 = self.augment_frames(frames)
            frames_aug2 = None
            if self.double_view:
                frames_aug2 = self.augment_frames(frames)

            # 预处理帧
            frames_aug1 = self.preprocess_frames(frames_aug1)
            if self.double_view:
                frames_aug2 = self.preprocess_frames(frames_aug2)

            X1_batch.append(frames_aug1)
            if self.double_view:
                X2_batch.append(frames_aug2)
            y_batch.append(label)

        X1_batch = np.array(X1_batch)
        y_batch = np.array(y_batch)

        if self.double_view:
            X2_batch = np.array(X2_batch)
            return (X1_batch, X2_batch), y_batch
        else:
            return X1_batch, y_batch

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.dataset_paths))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_paths, batch_labels):
        X = []
        y = []
        for i, video_path in enumerate(batch_paths):
            frames = self.load_frames(video_path)
            if self.split == 'train' and self.augment:
                # 生成两个增强版本
                frames_1 = self.augment_frames(frames)
                frames_2 = self.augment_frames(frames)
                frames_1 = self.preprocess_frames(frames_1)
                frames_2 = self.preprocess_frames(frames_2)
                X.append(frames_1)
                y.append(batch_labels[i])
                X.append(frames_2)
                y.append(batch_labels[i])
            else:
                frames = self.preprocess_frames(frames)
                X.append(frames)
                y.append(batch_labels[i])
        X = np.array(X)
        y = np.array(y)
        return X, y

    def augment_frames(self, frames):
        augmented_frames = []
        for frame in frames:
            # 随机颜色抖动
            frame = self.color_jitter(frame)
            # 随机灰度化
            if random.random() < 0.2:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            # 随机高斯模糊
            if random.random() < 0.5:
                frame = cv2.GaussianBlur(frame, (5, 5), 0)
            # 随机水平翻转
            if random.random() < 0.5:
                frame = cv2.flip(frame, 1)
            augmented_frames.append(frame)
        return np.array(augmented_frames)

    def load_frames(self, video_path):
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        if total_frames <= 0:
            # Handle cases where CAP_PROP_FRAME_COUNT is not available
            # Fallback to reading frames one by one (less efficient)
            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
            cap.release()
            frames = np.array(frames).astype(np.uint8)
            total_frames = len(frames)
            if total_frames < CLIP_LEN:
                # Repeat frames to reach CLIP_LEN
                repeat_times = (CLIP_LEN - total_frames) // total_frames + 1
                frames = np.tile(frames, (repeat_times, 1, 1, 1))[:CLIP_LEN]
            else:
                # Uniformly sample CLIP_LEN frames
                indices = np.linspace(0, total_frames - 1, CLIP_LEN).astype(int)
                frames = frames[indices]
        else:
            # Read only the required frames
            frames = []
            if total_frames < CLIP_LEN:
                # Read all frames and repeat to reach CLIP_LEN
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    frames.append(frame)
                frames = np.array(frames).astype(np.uint8)
                repeat_times = (CLIP_LEN - total_frames) // total_frames + 1
                frames = np.tile(frames, (repeat_times, 1, 1, 1))[:CLIP_LEN]
            else:
                # Calculate frame indices to sample
                indices = np.linspace(0, total_frames - 1, CLIP_LEN).astype(int)
                for idx in indices:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    if not ret:
                        break
                    frames.append(frame)
                frames = np.array(frames).astype(np.uint8)
        cap.release()
        return frames

    def preprocess_frames(self, frames):
        # 包含 resize、normalize、to_tensor 等操作
        frames = self.crop(frames, CLIP_LEN, CROP_SIZE, size2)
        frames = self.resize(frames)
        frames = self.normalize(frames)
        frames = self.to_tensor(frames)
        return frames


    def resize(self, frames):
        resized_frames = []
        for frame in frames:
            frame = cv2.resize(frame, (112, 112), interpolation=cv2.INTER_LINEAR)
            resized_frames.append(frame)
        return np.array(resized_frames)

    def normalize(self, frames):
        return frames.astype(np.float32) / 255.0

    def to_tensor(self, frames):
        return frames

    def random_flip(self, frames):
        if random.random() < 0.5:
            frames = frames[:, :, ::-1, :]
        return frames

    def crop(self, frames, clip_len, crop_size, crop_size2):
        if frames.shape[0] > clip_len:
            time_index = random.randint(0, frames.shape[0] - clip_len)
        else:
            time_index = 0
        height_index = random.randint(0, frames.shape[1] - crop_size)
        width_index = random.randint(0, frames.shape[2] - crop_size2)
        frames = frames[time_index:time_index + clip_len, height_index:height_index + crop_size,
                 width_index:width_index + crop_size2, :]
        if frames.shape[0] < clip_len:
            pad_num = clip_len - frames.shape[0]
            frames = np.concatenate((frames, frames[:pad_num]), axis=0)
        return frames

    def color_jitter(self, frame):
        # 转换为 PIL 图像
        img = Image.fromarray(frame)

        # 随机改变亮度
        if random.random() < 0.8:
            brightness_factor = random.uniform(0.6, 1.4)
            img = ImageEnhance.Brightness(img).enhance(brightness_factor)

        # 随机改变对比度
        if random.random() < 0.8:
            contrast_factor = random.uniform(0.6, 1.4)
            img = ImageEnhance.Contrast(img).enhance(contrast_factor)

        # 随机改变饱和度
        if random.random() < 0.8:
            saturation_factor = random.uniform(0.6, 1.4)
            img = ImageEnhance.Color(img).enhance(saturation_factor)

        # 随机改变色相（色调）
        if random.random() < 0.8:
            hue_factor = random.uniform(-0.1, 0.1)
            img = np.array(img.convert('HSV'))
            img[:, :, 0] = (img[:, :, 0].astype(int) + int(hue_factor * 255)) % 255
            img = Image.fromarray(img, mode='HSV').convert('RGB')

        # 转换回 NumPy 数组
        frame = np.array(img)

        return frame

    def gaussian_blur(self, frame):
        # 转换为 PIL 图像
        img = Image.fromarray(frame)

        # 以一定概率应用高斯模糊
        if random.random() < 0.5:
            radius = random.uniform(0.1, 2.0)
            img = img.filter(ImageFilter.GaussianBlur(radius))

        # 转换回 NumPy 数组
        frame = np.array(img)

        return frame


model_train_r2plus1d_18.py:
from tensorflow.keras import layers, models


def r2plus1d_block(input_tensor, filters, strides=(1, 1, 1)):
    # 2D 空间卷积
    x = layers.Conv3D(filters, kernel_size=(1, 3, 3), strides=strides, padding='same', use_bias=False)(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    # 1D 时间卷积
    x = layers.Conv3D(filters, kernel_size=(3, 1, 1), strides=(1, 1, 1), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    return x


def r2plus1d_residual_block(input_tensor, filters, strides=(1, 1, 1)):
    x = r2plus1d_block(input_tensor, filters, strides)
    x = r2plus1d_block(x, filters)

    # 跳跃连接
    shortcut = input_tensor
    if strides != (1, 1, 1) or input_tensor.shape[-1] != filters:
        shortcut = layers.Conv3D(filters, kernel_size=1, strides=strides, padding='same', use_bias=False)(input_tensor)
        shortcut = layers.BatchNormalization()(shortcut)

    x = layers.Add()([x, shortcut])
    x = layers.ReLU()(x)

    return x


def build_r2plus1d_model(input_shape=(16, 112, 112, 3), num_classes=2, feature_dim=512, include_top=False):
    inputs = layers.Input(shape=input_shape)

    # 初始卷积和池化层
    x = layers.Conv3D(64, kernel_size=(1, 7, 7), strides=(1, 2, 2), padding='same', use_bias=False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling3D(pool_size=(1, 3, 3), strides=(1, 2, 2), padding='same')(x)

    # 残差块
    x = r2plus1d_residual_block(x, 64)
    x = r2plus1d_residual_block(x, 64)

    x = r2plus1d_residual_block(x, 128, strides=(2, 2, 2))
    x = r2plus1d_residual_block(x, 128)

    x = r2plus1d_residual_block(x, 256, strides=(2, 2, 2))
    x = r2plus1d_residual_block(x, 256)

    x = r2plus1d_residual_block(x, 512, strides=(2, 2, 2))
    x = r2plus1d_residual_block(x, 512)

    # 全局平均池化
    x = layers.GlobalAveragePooling3D()(x)

    # 特征投影头（用于对比学习）
    features = layers.Dense(feature_dim, activation='relu')(x)

    if include_top:
        # 分类层（如果需要）
        outputs = layers.Dense(num_classes, activation='softmax')(features)
        model = models.Model(inputs, outputs)
    else:
        model = models.Model(inputs, features)

    return model


def load_r2plus1d_model(input_shape=(16, 112, 112, 3), feature_dim=512, include_top=False):
    model = build_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim, include_top=include_top)
    return model


def build_sscl_r2plus1d_model(input_shape=(16, 112, 112, 3), num_classes=2, feature_dim=512, include_top=False):
    inputs = layers.Input(shape=input_shape)

    # 初始卷积和池化层
    x = layers.Conv3D(64, kernel_size=(1, 7, 7), strides=(1, 2, 2), padding='same', use_bias=False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling3D(pool_size=(1, 3, 3), strides=(1, 2, 2), padding='same')(x)

    # 残差块
    x = r2plus1d_residual_block(x, 64)
    x = r2plus1d_residual_block(x, 64)

    x = r2plus1d_residual_block(x, 128, strides=(2, 2, 2))
    x = r2plus1d_residual_block(x, 128)

    x = r2plus1d_residual_block(x, 256, strides=(2, 2, 2))
    x = r2plus1d_residual_block(x, 256)

    x = r2plus1d_residual_block(x, 512, strides=(2, 2, 2))
    x = r2plus1d_residual_block(x, 512)

    # 全局平均池化
    x = layers.GlobalAveragePooling3D()(x)

    # 特征投影头（用于对比学习）
    features = layers.Dense(feature_dim, activation='relu')(x)

    projections = layers.Dense(feature_dim)(features)

    if include_top:
        # 分类层（如果需要）
        outputs = layers.Dense(num_classes, activation='softmax')(projections)
        model = models.Model(inputs, outputs)
    else:
        model = models.Model(inputs, projections)

    return model

def load_sscl_r2plus1d_model(input_shape=(16, 112, 112, 3), feature_dim=512, include_top=False):
    model = build_sscl_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim, include_top=include_top)
    return model


main.ipynb(script version)：
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import random
import numpy as np
import tensorflow as tf

from tensorflow.keras import layers, models, losses, optimizers, metrics

from model_train import (
    load_c3d_model,
    train_msupcl_model,
    linear_evaluation,
    load_c3d_sscl_model,
    train_simclr_model,
    linear_evaluation_sscl,
    supervised_contrastive_loss,
    nt_xent_loss,
)
from data_uniform_sup import VideoDataGenerator
from data_uniform_sscl import SSCLVideoDataGenerator
from paired_generator import PairedDataGenerator
from model_train_r2plus1d_18 import (
    load_r2plus1d_model,
    load_sscl_r2plus1d_model,
)


# In[2]:


seed = 2042
np.random.seed(seed)
random.seed(seed)
tf.random.set_seed(seed)

input_shape = (12, 64, 64, 3)  # 数据生成器中定义的输入形状
num_classes = 2  # 有害内容或安全内容
feature_dim = 512
num_epochs = 3
batch_size = 4
temperature = 0.8
learning_rate = 0.001


# In[3]:


# Define dataset paths
violence_negative_dir = './data/violence_dataset/NonViolence'
violence_positive_dir = './data/violence_dataset/Violence'
tiktok_negative_dir = './data/tiktok/train/Safe'
tiktok_positive_dir = './data/tiktok/train/Harmful Content'


# In[4]:


# 定义函数用于采样视频
def sample_videos(directory, num_samples=100):
    all_videos = [
        os.path.join(directory, f)
        for f in os.listdir(directory)
        if f.endswith('.mp4')
    ]
    sampled_videos = random.sample(all_videos, min(num_samples, len(all_videos)))
    return sampled_videos


# In[5]:


# Violence dataset
violence_negative_videos = sample_videos(violence_negative_dir, 50)
violence_positive_videos = sample_videos(violence_positive_dir, 50)

# TikTok dataset
tiktok_negative_videos = sample_videos(tiktok_negative_dir, 50)
tiktok_positive_videos = sample_videos(tiktok_positive_dir, 50)


# In[6]:


def split_data(negative_videos, positive_videos, train_ratio=0.55, val_ratio=0.15):
    # 合并并打乱数据
    videos = negative_videos + positive_videos
    labels = [0] * len(negative_videos) + [1] * len(positive_videos)
    combined = list(zip(videos, labels))
    random.shuffle(combined)
    videos[:], labels[:] = zip(*combined)

    # 计算划分索引
    total = len(videos)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)

    # 划分数据集
    train_videos = videos[:train_end]
    train_labels = labels[:train_end]
    val_videos = videos[train_end:val_end]
    val_labels = labels[train_end:val_end]
    test_videos = videos[val_end:]
    test_labels = labels[val_end:]

    return (train_videos, train_labels), (val_videos, val_labels), (test_videos, test_labels)



# In[7]:


# Violence dataset
(
    (violence_train_videos, violence_train_labels),
    (violence_val_videos, violence_val_labels),
    (violence_test_videos, violence_test_labels),
) = split_data(violence_negative_videos, violence_positive_videos)


# TikTok dataset
(
    (tiktok_train_videos, tiktok_train_labels),
    (tiktok_val_videos, tiktok_val_labels),
    (tiktok_test_videos, tiktok_test_labels),
) = split_data(tiktok_negative_videos, tiktok_positive_videos)


# In[8]:


# Convert labels to numpy arrays and one-hot encode them if necessary
def prepare_labels(labels):
    return np.array(labels)


violence_train_labels_np = prepare_labels(violence_train_labels)
violence_val_labels_np = prepare_labels(violence_val_labels)
violence_test_labels_np = prepare_labels(violence_test_labels)


tiktok_train_labels_np = prepare_labels(tiktok_train_labels)
tiktok_val_labels_np = prepare_labels(tiktok_val_labels)
tiktok_test_labels_np = prepare_labels(tiktok_test_labels)


# In[9]:


# Violence dataset generators
violence_train_generator = VideoDataGenerator(
    violence_train_videos,
    violence_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
)
violence_val_generator = VideoDataGenerator(
    violence_val_videos,
    violence_val_labels_np,
    batch_size=batch_size,
    shuffle=False,
)
violence_test_generator = VideoDataGenerator(
    violence_test_videos,
    violence_test_labels_np,
    batch_size=batch_size,
    shuffle=False,
)


# In[10]:


# TikTok dataset generators
tiktok_train_generator = VideoDataGenerator(
    tiktok_train_videos,
    tiktok_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
)
tiktok_val_generator = VideoDataGenerator(
    tiktok_val_videos,
    tiktok_val_labels_np,
    batch_size=batch_size,
    shuffle=False,
)
tiktok_test_generator = VideoDataGenerator(
    tiktok_test_videos,
    tiktok_test_labels_np,
    batch_size=batch_size,
    shuffle=False,
)


# In[11]:


# Load the model
base_model = load_c3d_model(input_shape=input_shape, feature_dim=feature_dim)
print("Base Model Summary:")
base_model.summary()


# In[12]:


def create_classification_model(base_model, num_classes):
    features = base_model.output
    outputs = layers.Dense(num_classes, activation='softmax')(features)
    model = models.Model(inputs=base_model.input, outputs=outputs)
    return model


# In[13]:


classification_model_violence = create_classification_model(base_model, num_classes)
# 冻结基础模型的参数
for layer in classification_model_violence.layers[:-1]:
    layer.trainable = False

classification_model_violence.compile(
    loss=losses.SparseCategoricalCrossentropy(),
    optimizer=optimizers.Adam(learning_rate=learning_rate),
    metrics=[metrics.SparseCategoricalAccuracy()],
)


# In[14]:


print("Classification Model for Violence Dataset Summary:")
classification_model_violence.summary()


# In[15]:


history_violence = classification_model_violence.fit(
    violence_train_generator,
    validation_data=violence_val_generator,
    epochs=num_epochs,

)


# In[34]:


# Evaluate on Violence test set
base_c3d_results_violence = classification_model_violence.evaluate(violence_test_generator)
print(
    f"Violence Dataset - Test Loss: {base_c3d_results_violence[0]}, Test Accuracy: {base_c3d_results_violence[1]}"
)


# In[35]:


classification_model_tiktok = create_classification_model(base_model, num_classes)

for layer in classification_model_tiktok.layers[:-1]:
    layer.trainable = False

classification_model_tiktok.compile(
    loss=losses.SparseCategoricalCrossentropy(),
    optimizer=optimizers.Adam(learning_rate=learning_rate),
    metrics=[metrics.SparseCategoricalAccuracy()],
)
print("Classification Model for TikTok Dataset Summary:")
classification_model_tiktok.summary()


# In[36]:


history_tiktok = classification_model_tiktok.fit(
    tiktok_train_generator,
    validation_data=tiktok_val_generator,
    epochs=num_epochs,
)


# In[37]:


# Evaluate on TikTok test set
base_c3d_results_tiktok = classification_model_tiktok.evaluate(tiktok_test_generator)
print(
    f"TikTok Dataset - Test Loss: {base_c3d_results_tiktok[0]}, Test Accuracy: {base_c3d_results_tiktok[1]}"
)


# ## MSupCL implementation

# In[16]:


# Combine training data from both datasets
combined_train_videos = violence_train_videos + tiktok_train_videos
combined_train_labels = violence_train_labels_np.tolist() + tiktok_train_labels_np.tolist()


# Create a combined data generator
combined_train_generator = VideoDataGenerator(
    combined_train_videos,
    combined_train_labels,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
)

violence_train_generator_no_aug = VideoDataGenerator(
    violence_train_videos,
    violence_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=False,
)

tiktok_train_generator_no_aug = VideoDataGenerator(
    tiktok_train_videos,
    tiktok_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=False,
)

paired_train_generator = PairedDataGenerator(
    violence_train_generator_no_aug,
    tiktok_train_generator_no_aug,
    batch_size=32,
)


# In[39]:


msupcl_model = load_c3d_model(input_shape=input_shape, feature_dim=feature_dim)
print("MSupCL Model Summary:")
msupcl_model.summary()


# In[40]:


# Train the model
train_msupcl_model(msupcl_model, paired_train_generator, epochs=num_epochs, temperature=temperature)




# In[41]:


msupcl_c3d_result_violence, msupcl_c3d_result_tiktok = linear_evaluation(
    msupcl_model,
    combined_train_generator,
    violence_test_generator,
    tiktok_test_generator,
    num_classes=num_classes,
)


# ## SSCL
#

# In[42]:


violence_train_sscl_generator = SSCLVideoDataGenerator(
    violence_train_videos,
    violence_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
)

violence_val_sscl_generator = SSCLVideoDataGenerator(
    violence_val_videos,
    violence_val_labels_np,
    batch_size=batch_size,
    shuffle=False,
    augment=False,
    double_view=False,
)
violence_test_sscl_generator = SSCLVideoDataGenerator(
    violence_test_videos,
    violence_test_labels_np,
    batch_size=batch_size,
    shuffle=False,
    augment=False,
    double_view=False,
)
tiktok_train_sscl_generator = SSCLVideoDataGenerator(
    tiktok_train_videos,
    tiktok_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
)
tiktok_val_sscl_generator = SSCLVideoDataGenerator(
    tiktok_val_videos,
    tiktok_val_labels_np,
    batch_size=batch_size,
    shuffle=False,
    augment=False,
    double_view=False,
)
tiktok_test_sscl_generator = SSCLVideoDataGenerator(
    tiktok_test_videos,
    tiktok_test_labels_np,
    batch_size=batch_size,
    shuffle=False,
    augment=False,
    double_view=False,
)



# In[43]:


sscl_model = load_c3d_sscl_model(input_shape=input_shape, feature_dim=feature_dim)
print("SSCL Model Summary:")
sscl_model.summary()


# In[44]:


# 训练SSCL模型（在暴力数据集上）
train_simclr_model(sscl_model, violence_train_sscl_generator, epochs=num_epochs, temperature=temperature)


# In[ ]:


# 在线性评估中使用训练好的SSCL模型（暴力数据集）
sscl_c3d_result_violence = linear_evaluation_sscl(
    sscl_model,
    violence_train_sscl_generator,
    violence_val_sscl_generator,
    violence_test_sscl_generator,
    num_classes=num_classes,
)


# In[ ]:


train_simclr_model(sscl_model, tiktok_train_sscl_generator, epochs=num_epochs, temperature=temperature)


# In[ ]:


sscl_c3d_result_tiktok = linear_evaluation_sscl(
    sscl_model,
    tiktok_train_sscl_generator,
    tiktok_val_sscl_generator,
    tiktok_test_sscl_generator,
    num_classes=num_classes,
)


# ## R2+1d_18 model

# In[20]:






# In[21]:


base_model = load_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim, include_top=False)


# In[22]:


for layer in base_model.layers:
    layer.trainable = False

features = base_model.output
outputs = layers.Dense(num_classes, activation='softmax')(features)
classification_model = models.Model(inputs=base_model.input, outputs=outputs)

classification_model.compile(
    loss=losses.SparseCategoricalCrossentropy(),
    optimizer=optimizers.Adam(learning_rate=learning_rate),
    metrics=[metrics.SparseCategoricalAccuracy()]
)


# In[23]:


# Train on Violence dataset
history_violence = classification_model.fit(
    violence_train_generator,
    validation_data=violence_val_generator,
    epochs=num_epochs
)


# In[24]:


# Evaluate on Violence test set
base_r2plus1d_results_violence = classification_model.evaluate(violence_test_generator)
print(f"Violence Dataset - Test Loss: {base_r2plus1d_results_violence[0]}, Test Accuracy: {base_r2plus1d_results_violence[1]}")


# In[25]:


history_tiktok = classification_model.fit(
    tiktok_train_generator,
    validation_data=tiktok_val_generator,
    epochs=num_epochs
)


# In[26]:


# Evaluate on TikTok test set
base_r2plus1d_results_tiktok = classification_model.evaluate(tiktok_test_generator)
print(f"TikTok Dataset - Test Loss: {base_r2plus1d_results_tiktok[0]}, Test Accuracy: {base_r2plus1d_results_tiktok[1]}")


# In[27]:


msupcl_model = load_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim, include_top=False)


# In[28]:


msupcl_c3d_result_violence, msupcl_c3d_result_tiktok = linear_evaluation(
    msupcl_model,
    combined_train_generator,
    violence_test_generator,
    tiktok_test_generator,
    num_classes=num_classes,
)

violence_train_generator_no_aug = VideoDataGenerator(violence_train_videos, violence_train_labels_np, batch_size=batch_size, shuffle=True, augment=False)
tiktok_train_generator_no_aug = VideoDataGenerator(tiktok_train_videos, tiktok_train_labels_np, batch_size=batch_size, shuffle=True, augment=False)
paired_train_generator = PairedDataGenerator(violence_train_generator_no_aug, tiktok_train_generator_no_aug)


# In[29]:


train_msupcl_model(msupcl_model, paired_train_generator, epochs=num_epochs)


# In[30]:


msupcl_r2plus1d_result_violence, msupcl_r2plus1d_result_tiktok = linear_evaluation(msupcl_model, combined_train_generator, violence_test_generator,tiktok_test_generator, num_classes=num_classes)


# # SSCL

# In[10]:


violence_train_sscl_generator = SSCLVideoDataGenerator(
    violence_train_videos,
    violence_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True
)

violence_val_single_sscl_generator = SSCLVideoDataGenerator(
    violence_val_videos,
    violence_val_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
    double_view=False
)


violence_test_sscl_generator = SSCLVideoDataGenerator(
    violence_test_videos,
    violence_test_labels_np,
    batch_size=batch_size,
    shuffle=False,
    augment=False,
    double_view=False
)

tiktok_train_sscl_generator = SSCLVideoDataGenerator(
    tiktok_train_videos,
    tiktok_train_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True
)

tiktok_val_single_sscl_generator = SSCLVideoDataGenerator(
    tiktok_val_videos,
    tiktok_val_labels_np,
    batch_size=batch_size,
    shuffle=True,
    augment=True,
    double_view=False
)

tiktok_test_sscl_generator = SSCLVideoDataGenerator(
    tiktok_test_videos,
    tiktok_test_labels_np,
    batch_size=batch_size,
    shuffle=False,
    augment=False,
    double_view=False
)

violence_train_single_sscl_generator = SSCLVideoDataGenerator(violence_train_videos,violence_train_labels_np,batch_size=batch_size, shuffle=True, augment=True, double_view=False)


tiktok_train_single_sscl_generator = SSCLVideoDataGenerator(tiktok_train_videos,tiktok_train_labels_np,batch_size=batch_size, shuffle=True, augment=True, double_view=False)


# In[31]:


model = load_c3d_sscl_model(input_shape=input_shape, feature_dim=feature_dim)

train_simclr_model(model, violence_train_sscl_generator, epochs=num_epochs)


# In[32]:


sscl_c3d_result_violence = linear_evaluation_sscl(model, violence_train_single_sscl_generator, violence_val_single_sscl_generator,violence_test_sscl_generator, num_classes=num_classes)


# In[33]:


train_simclr_model(model, tiktok_train_sscl_generator, epochs=num_epochs, temperature=temperature)
sscl_c3d_result_tiktok = linear_evaluation_sscl(model, tiktok_train_single_sscl_generator,tiktok_val_single_sscl_generator,tiktok_test_sscl_generator, num_classes=num_classes)


# In[13]:


model = load_sscl_r2plus1d_model(input_shape=input_shape, feature_dim=feature_dim)
train_simclr_model(model, violence_train_sscl_generator, epochs=num_epochs, temperature=temperature)


# In[35]:


sscl_r2plus1d_result_violence = linear_evaluation_sscl(model, violence_train_single_sscl_generator, violence_val_single_sscl_generator,violence_test_sscl_generator, num_classes=num_classes)


# In[36]:


train_simclr_model(model, tiktok_train_sscl_generator, epochs=num_epochs, temperature=temperature)
sscl_r2plus1d_result_tiktok = linear_evaluation_sscl(model, tiktok_train_single_sscl_generator,tiktok_val_single_sscl_generator,tiktok_test_sscl_generator, num_classes=num_classes)





论文原文：

This WACV paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.




Activity-based Early Autism Diagnosis Using
A Multi-Dataset Supervised Contrastive Learning Approach


Asha Rani
IIT Jodhpur, India
rani.1@iitj.ac.in

Yashaswi Verma IIT Jodhpur, India
yashaswi@iitj.ac.in



Abstract
Autism Spectrum Disorder (ASD) is a neurological disorder. Its primary symptoms include difficulty in verbal/non-verbal communication and rigid/repetitive be- havior. Traditional methods of autism diagnosis require multiple visits to a human specialist. However, this process is generally time-consuming and may result in a delayed (early) intervention. In this paper, we present a data-driven approach to automate autism diagnosis using video clips of subjects performing simple activities recorded in a weakly constrained environment. This task is particularly challeng- ing since the available training data is small, videos from the two categories (“ASD” and “Control”) are generally perceptually indistinguishable, and there is no clear under- standing of what features would be beneficial in this task. To address these, we present a novel multi-dataset supervised contrastive learning technique to learn discriminative fea- tures simultaneously from multiple video datasets with sig- nificantly diverse distributions. Extensive empirical analy- ses demonstrate the promise of our approach compared to competing techniques on this challenging task.


1.	Introduction
Autism Spectrum Disorder (ASD), commonly known as autism, is a neurological disorder that is primarily charac- terized by difficulty in social interaction. ASD affects a per- son’s ability to communicate properly with others and is re- ported to occur across all racial, ethnic and socio-economic groups. The symptoms of ASD usually start appearing in early childhood between the age of 1-3 years. These in- clude a lack of proper eye contact, poor imitation skills and rigid/repetitive behavior. While the number of identi- fied ASD cases has increased in the last decade, the current mean age of ASD diagnosis is reported as 3-12 years [18]. In the recent times, several measures have been adopted to reduce the mean age of diagnosis. This is particularly cru- cial since an early diagnosis of ASD leads to an early inter-




Figure 1. Sample frames from video clips of subjects from the ASD (top) and Control (bottom) categories in the Hand Gesture dataset [32]. Note that these video clips exhibit low inter-class variability and are difficult to classify using individual visual cues.


vention and subsequent therapies that significantly benefit the growth of the affected child. Traditionally, ASD diag- nosis has been performed physically by an expert medical practitioner. However, this requires multiple visits and is thus time-consuming as well as sometimes error-prone.
The limitations of physical ASD diagnosis can be ad- dressed to a large extent by adopting an automated ap- proach. In this paper, we present a data-driven approach for automated autism diagnosis using recorded activity (ac- tion) video clips of subjects. Each clip captures a subject performing some specific activity in a loosely controlled en- vironment, and is much easier and less expensive to acquire compared to data in other modalities such as EEG, MRI or eye-tracking. However, it is difficult to obtain a good prediction accuracy with a machine learning model trained on such data because (a) the training data contains a small number of samples, (b) the data is inherently complex to comprehend, and (c) the video samples (clips) exhibit low inter-class variability. E.g., Figure 1 shows sample video frames corresponding to “ASD” and “Control” category in one such dataset (the Hand Gesture dataset [32]), where we can observe that both the samples look quite similar. Due to this, it is difficult to correctly classify samples based on in- dividual visual cues. In our experiments also, we will show that a conventional deep neural network can not be directly adopted for this task for the same reasons.
To address these challenges, we propose to use con- trastive feature learning [3, 12, 20] to learn distinctions be-

tween video clips of the two categories (ASD and Con- trol) in a relative manner. Specifically, we present a novel multi-dataset supervised contrastive learning (MSupCL) technique that learns discriminative features by simultane- ously using multiple (activity-based) video datasets from diverse distributions. Thorough empirical experiments on the two relevant and publicly available datasets (Hand Gesture [32] and Autism [19]) show that our proposed approach significantly outperforms competing contrastive learning techniques [3, 12, 20] on the challenging Hand Gesture dataset [32], and is comparable to the best ap- proach on the (easier) Autism dataset [19]. For repro- ducibility, our code and pre-trained models are available at https://github.com/asharani97/MDSupCL.
2.	Related Work
Several machine learning based techniques have been proposed in the recent years that aim at performing autism diagnosis in an automated manner. In general, most of these efforts have primarily investigated data acquired in the form of different modalities and posed it as a single-step classi- fier learning task [9, 11, 13, 15, 17, 21–26, 31, 32]. One of the early studies on autism diagnosis revealed that autis- tic individuals have atypical sight [6]. Later, the authors of [11, 30] further worked on this finding, with [11] focus- ing on visual bias towards different objects, their contrast and colour, while [30] focusing on using these aspects to predict ASD and control subjects using deep features [16]. Another approach to distinguish between the two categories was introduced in [22], where the first-person viewpoint of a scene by an individual is compared for analysis. A few studies that have investigated other modalities of data in- clude eye-gaze data to distinguish on the basis of visual at- tention [26], EEG signal [1, 2, 25], and MRI data [9, 13]. Since these different modalities of data are difficult and ex- pensive to acquire, a recent work [32] introduced the Hand Gesture dataset which contains short video clips that cap- ture predefined activities performed by autistic/control sub- jects in a weakly controlled environment.
It is worth noting that while most of the existing machine learning based autism diagnosis approaches have analyzed the pros and cons of using features from different modali- ties as discussed above, eventually they pose this as a binary classification task and use a sample-level binary classifier such as SVM or a deep neural network. Unlike existing ap- proaches, we aim at learning discriminative features in a rel- ative manner by simultaneously using two diverse datasets in a contrastive learning based set-up. The broad idea of learning from multiple datasets simultaneously has recently gained popularity [5, 28], where a model is trained by inte- grating multiple datasets created for a specific task such as object detection [5], image segmentation [28], etc. In gen- eral, multi-dataset training aims at learning improved fea-

ture representations given the costly nature of annotation for diverse downstream applications. This also helps in mini- mizing the issue of domain shift across different datasets (also see our discussion in Section 4.4.1). As per our knowl- edge, ours is the first NT-Xent (normalized temperature- scaled cross entropy) loss based contrastive learning ap- proach for a multi-dataset set-up, as well as the the first such attempt for the activity video-based autism diagnosis task.

3.	Proposed Approach
Our approach consists of two steps: learning a deep fea- ture encoder network using the proposed multi-dataset su- pervised contrastive learning approach, followed by train- ing of a dataset-specific classifier for prediction.
3.1.	Background and Motivation
Contrastive learning requires positive/similar and nega- tive/dissimilar pairs of samples, and aims at pulling similar samples closer than dissimilar samples in the learned fea- ture space. Our approach is motivated by the success of the recent contrastive learning techniques such as [3,4,8,12,20]. SSCL [3] is one of the earlier uni-modal approaches that is based on self-supervised contrastive learning. In this, a positive pair is generated using a given sample (anchor) and its transformed version (obtained using some non-learned transformation; e.g., horizontal flip), and a negative pair is generated using the same given sample and any other sam- ple (either from the dataset or the transformed version of another sample). Extending SSCL, SupCL [12] uses ex- ternal supervision in the form of category labels for creat- ing positive pairs, and for each positive pair. In the recent multi-modal self-supervised approach MSSCL [20], the in- put data contains paired samples from two different modal- ities (video and audio) that are extracted from an audio- video recording, thus naturally giving cross-modal positive pairs. However, it may not always be feasible to acquire paired data. Unlike MSSCL, our approach does not require an explicit pairing of samples. Rather, we take motivation from [20] and make use of the categorical information to create multiple cross-dataset positive pairs corresponding to each anchor. Specifically, we assume that two datasets may be collected independently and follow different dis- tributions, however both should contain samples from the same set of categories. This allows us to create cross-dataset positive pairs of samples based on categorical supervision without requiring an explicit pairing as in [20]. Below, we describe our first step, i.e., generation of pairs for our con- trastive learning based approach.
3.2.	Pair Generation
In our task, we have two datasets D1 = {(vi, yi)} and
D2 = {(vj, yj)}, each containing activity videos of ASD



Figure 2. Block diagram of our network architecture and training process. We consider two different datasets D1 and D2 for the same task (i.e. ASD diagnosis in our case) from different distributions. In a batch, there are samples from both the datasets. For cross-dataset pair generation (Section 3.2), one sample is picked from each dataset (a transformation is applied on only one of these samples). Both the samples are fed to the encoder network e() to generate the initial feature representations, which are then passed through a multi-layer perceptron head h() giving us the embeddings zi and zj respectively (Section 3.3). These embeddings are then used to compute the proposed MSupCL loss (Section 3.4) and train the whole network in an end-to-end manner using a gradient descent method.


and control subjects collected separately; i.e., by indepen- dent groups of researchers/clinicians under different con- ditions. During training, a batch consists of samples from both the datasets. In a batch, for a given sample (anchor) from one dataset, we create a positive cross-dataset pair by pairing it with a sample from the second dataset that belongs to the same category. To create a negative cross-dataset pair for the same anchor point, we pair it with a sample from the second dataset that belongs to another category. Specif- ically, consider an anchor point and its corresponding cat- egory (va, ya) from one dataset. To create a positive pair, we pick a sample (vp, yp) from the second dataset such that ya = yp. To create a negative pair, we pick another sample (vn, yn) from the second dataset such that ya ̸= yn. We perform this to obtain all possible pairs in the given batch, thus resulting in multiple cross-dataset positive and nega- tive pairs corresponding to each anchor point. These pairs are then passed to a deep feature extraction (encoder) net- work, as described next.

zi and zj respectively (Figure 2). To learn the parameters of this network, we propose a novel multi-dataset supervised contrastive loss function, which we describe next.
3.4.	Multi-dataset Supervised Contrastive Loss
For a given anchor point va, we create a positive pair (va, vp) and a negative pair (va, vn) as discussed in the pair generation step. Let Pa and Na denote the sets of all such positive and negative samples with respect to va, which are used to create positive and negative pairs respectively. These pairs are passed through the feature extraction net- work as described above to obtain their feature embeddings. For the anchor va, its positive sample vp and its negative sample vn, the feature embeddings are denoted by za, zp and zn respectively. Also, let Ka = Pa ∪ Na be the set of all such positive and negative samples corresponding to va. Using these, we calculate the multi-dataset supervised contrastive loss for va as below:


3.3.	Feature Extraction Network

a MSupCL

 1
= −
|Pa|

汇
vp∈Pa

exp(za · zp/τ )
log
vk∈Ka exp(za·zk/τ )

(1)

Given a (positive/negative) pair of videos vi and vj cre- ated from the datasets D1 and D2 respectively, we pass them through feature encoder networks ei() and ej() to obtain their initial feature representations xi and xj respectively. These feature representations are then passed through fully- connected layers hi() and hj() and mapped to embeddings

Here, τ denotes the temperature parameter, and the · symbol denotes the inner (dot) product. This loss is averaged over all the samples in a batch by considering each sample as an anchor point at a time to obtain the total loss. This is then used to train the whole network (Figure 2) in an end-to-end manner using a gradient descent approach (Section 4.3).


(a) SSCL [3]	(b) SupCL [12]	(c) MSSCL [20]	(d) MSupCL (Ours)
Figure 3. Schematic comparison of the proposed MSupCL approach with three contrastive learning based methods in terms of network architectures and training procedures. In all the methods, an input sample vi is first passed through an encoder network e() and then through a non-linear multi-layer perceptron head h() to get the embedding zi. t() denotes a transformation function that generates a sample v′ from vi using a simple transformation, which is then used to create a positive pair during training. Here: (a) SSCL is a self-supervised uni-modal technique that uses an NT-Xent loss. (b) SupCL is a label-supervision based uni-modal technique that uses a supervised contrastive loss.
(c) MSSCL is an extension of SSCL for multi-modal data Di ∪ Dj and requires explicit pairing between cross-modal samples during training. (d) MSupCL (ours) uses a multiple unpaired datasets simultaneously, however the pairing of data points is done based on label information instead of explicit pairing as in MSSCL, and is trained using the loss function as described in Section 3.4.


It is worth noting that the loss function in Eq. 1 gen- eralizes the supervised constrastive loss proposed in [12] to a multi-dataset set-up, and extends the multi-modal self- supervised contrastive loss proposed in [20] to benefit from categorical supervision. We would also like to emphasize that while we demonstrate our approach on a multi-dataset set-up, our approach may be easily adapted for data contain- ing different modalities by plugging appropriate modality- specific encoder networks.
3.5.	Classification
The next step is to train a dataset-specific classification model. Following earlier related papers [3, 12, 20], we take only the encoder network from the previously trained net- work (as shown in Figure 2) and add a new fully-connected classification layer with softmax activation. To train this layer of the updated network, we freeze the parameters of the encoder network and learn those in the classification layer using the standard cross-entropy loss. This network is then used for doing prediction on the unseen/test data. It should be noted that the classification network needs to be trained for each dataset individually [20].

4.	Experiments
In this section, we discuss our experimental setup and present the empirical results.
4.1.	Datasets
We use two activity video datasets in our experiments: Hand Gesture dataset [32] and Autism dataset [19].  As

per our knowledge, these are the only relevant and pub- licly accessible activity video datasets for this task. Both the datasets contain short video clips of subjects where they are asked to perform some predefined activities in a loosely controlled environment.
Hand Gesture dataset [32]: This dataset contains video recordings of four activities (placing, pouring, pass to place, and pass to pour), each performed multiple times by 39 sub- jects (19 ASD and 20 Control). As discussed in Section 1 and illustrated in Figure 1, this is a challenging dataset with high intra-class and low inter-class variability.
Autism dataset [19]: This dataset contains video record- ings corresponding to eight activities (move the table, touch ear, lock hands, touch head, touch nose, rolly polly, tapping, and arms up). Each activity is recorded from two camera orientations: tutor facing and child-facing. To create this dataset, the authors of [19] first recorded videos of ASD subjects, and then obtained samples of the “control” class by picking a similar number of videos corresponding to the most identical actions from the HMDB51 dataset [14]. Be- cause of this, the distributions of the two classes are well- separated (Figure 4) in this dataset.

4.2.	Compared Methods
To examine the effectiveness of the proposed MSupCL approach, we compare it with competing contrastive learn- ing techniques including the uni-modal self-supervised approach SSCL [3], uni-modal supervised approach SupCL [12], and multi-modal self-supervised approach MSSCL [20], as illustrated in Figure 3. As a baseline, we also compare with a standard deep classifier (BinClass)

Dataset	% Accuracy
Activity	#Samples	BinClass	SSCL	SupCL	MSSCL	MSupCL
Pass to Place	139	51.09	70.07	70.80	64.23	89.78
Pass to Pour	140	54.23	73.24	69.72	57.04	81.69
Placing	140	51.43	73.57	67.14	55.71	80.71
Pouring	142	52.11	68.31	69.01	52.11	85.92
Average	561	52.23	71.30	69.16	57.22	84.49


Table 1. Activity-wise and average classification accuracy on the Hand Gesture dataset. In each row, the best result is highlighted in bold, while the second best is underlined.


trained using the binary cross-entropy loss.
4.3.	Implementation Details
We first pre-process each activity video by extracting key frames while keeping the sequential information intact. We uniformly pick 16 and 10 frames from each video sample of the Hand Gesture and Autism dataset respectively, since these datasets contain 21-30 frames and 12-20 frames per video clip respectively. We use the ResNet-based R(2+1)D- 18 [27] network as the encoder network e() in all the com- pared methods, which is the most widely used feature en- coder for activity/action datasets. It maps an input video (a sequence of sampled frames) into a 512-dimensional fea- ture vector. We pass this feature vector through a fully- connected layer h() with ReLU activation, giving a 256- dimensional feature vector. This is then normalized using the L2-norm and is used to compute the loss function. In our case, since both the modalities are videos (though obtained from different sources and thus following different distribu- tions), we use a duplicated encoder network. For classifi- cation, we keep only the encoder network and add a fully- connected classification layer with softmax activation. For fair comparisons, we use the same approach for classifier training in the three contrastive learning techniques (SSCL, SupCL and MSSCL) as used in the proposed MSupCL. In all the experiments, we keep the train-test ratio as 70:30.
Compute Environment: The experiments were conducted on a server with shared access, having 8 GTX 1080 Ti 12GB GPUs, Intel Xeon E5-2650 2.20GHz processors, and 256GB RAM. For training MSSCL and MSupCL, 4 GPUs were used, while for other methods, 2 GPUs were used.
4.4.	Results and Discussion
We first compare the classification accuracy of all the methods on the Hand Gesture dataset in Table 1. We ob- serve that on this challenging dataset, the baseline binary classifier is insufficient to learn discriminative features, thus leading to a poor (near-chance) accuracy. Compared to this, both SupCL and SSCL achieve significantly higher accu- racy. Interestingly, we notice that SSCL performs slightly better than SupCL even without using categorical informa-



Table 2. Activity-wise and average classification accuracy on the Autism dataset. In each row, the best results is highlighted in bold, while the second best is underlined.


tion. We believe this is because of the inherent visual com- plexity of these task (perceptually indistinguishable vari- ations among samples from the two categories), coupled with the technical distinctions between the two approaches with respect to their loss functions and the way they cre- ate positive/negative pairs (for further details, we request the reader the refer to the respective papers). This also in- dicates that categorical information may not significantly affect the accuracy of uni-modal approaches in tasks like ours where the number of samples is small and datasets depict low inter-class variability. However, the perfor- mance drops drastically in case of MSSCL, which indicates that self-supervised learning may not benefit from uncou- pled/unpaired multi-source data as in our case (note that the original paper [20] used paired multi-modal data). For the same reason, we acknowledge that MSSCL is not directly comparable to our approach. Finally, we can notice that the proposed MSupCL approach achieves the maximum accu- racy, which is around 13% (absolute) more than the second- best method (SSCL). This indicates that multiple unpaired but labelled datasets can be effectively used for multi- dataset contrastive learning by creating (positive/negative) pairs based on categorical information, and thus validates the utility of our approach over the compared techniques.
Next, we compare the accuracy of all the methods on the Autism dataset in Table 2. As emphasized in Section 4.1, the distributions of the two categories in this dataset are well separated. This becomes evident in the empirical re- sults where all the compared methods achieve a near-perfect accuracy with statistically insignificant differences in their predictions (based on the t-test, with t-value 0.00).

4.4.1	Relation with Knowledge Distillation
Knowledge distillation [7, 10] is a well-studied idea where the broad objective is to transfer the knowledge of one or more teacher models to a student model (i.e., model-level knowledge distillation) for a particular dataset, where the teacher model is generally assumed to be of more/equal learning capacity compared to the student model. In our



Figure 4. t-SNE visualization of features learned using different methods on the Hand Gesture dataset (top row) and the Autism dataset (bottom row). The points in blue denote Control samples while those in red denote ASD samples. (Best viewed in colour.)


task, we have two datasets that belong to the same (video) modality, are developed for the same task (i.e., ASD diagno- sis), and also have the same output space (ASD versus Con- trol). However, these datasets are collected separately by different groups of researchers under different conditions. Due to this, they lie in completely different spaces and their sample distributions are non-comparable, thus making this a multi-dataset task. Further, one (Autism) dataset is easy for which we can train a model that achieves high accuracy (c.f., Table 2), and the other (Hand Gesture) dataset is difficult for which the accuracy is relatively low (c.f., Table 1), while considering the same network architecture (learning capac- ity) for both. The quantitative results discussed above indi- cate that while the classification accuracy is nearly saturated on the Autism dataset, it can contribute in boosting the accu- racy on the more challenging Hand Gesture dataset through our contrastive learning based multi-dataset approach. In other words, our approach seamlessly distills (extracts and propagates) task-specific knowledge from the easy dataset to the model trained for the difficult dataset and improves its accuracy, thus resulting in data-level knowledge distilla- tion. As per our knowledge, this is the first such attempt on this task, and we believe that our approach may also benefit other healthcare/biomedical applications.
4.5.	Analyses
The above results are supported by the t-SNE [29] vi- sualization of the features learned by different methods on the two datasets as shown in Figure 4. Here, we can ob-

serve that for the Hand Gesture dataset, the samples from the two classes (ASD and Control) are best separated using our MSupCL approach. However, for the Autism dataset, the samples are well-separated in all the cases, thus leading to a high classification accuracy.
Figure 5 shows the confusion matrix for all the meth- ods on the Hand Gesture dataset. (we do not include the confusion matrix for the Autism dataset as all the methods achieve a near perfect accuracy on this dataset with statisti- cally insignificant difference). We can observe that the bi- nary classifier classifies most of the samples as ASD, which means it cannot differentiate between the ASD and Control samples. On the other hand, while SupCL and MSSCL cor- rectly predict majority of Control samples, the number of incorrect ASD predictions is relatively higher. SSCL per- forms good for ASD samples but also misclassifies a large number of Control samples to ASD. MSupCL improves upon SSCL as it correctly classifies more number of Con- trol samples than any other method, while the number of correct predictions for ASD is comparable to SSCL.
4.6.	Ablation Study
In Figure 6, we analyze the impact of different hyper- parameters on MSupCL’s performance using the Hand Ges- ture dataset (we use the same set of hyper-parameters for both the datasets). In Figure 6(a), we first study the im- portance of the final feature embedding size (z) by varying it in the range {32, 64, 128, 256, 512, 1024}. We observe that initially the accuracy increases as we increase the em-


Figure 5. Confusion matrix for different methods on the Hand Gesture dataset. Here, ‘0’ corresponds to the Control category and ‘1’ corresponds to the ASD category.

Figure 6. Ablation study on the Hand Gesture dataset by varying different hyper-parameters. In each plot, the vertical axis denotes percentage classification accuracy.


bedding size, and then it starts to drop. Next, we study the impact of the temperature parameter τ in Figure 6(b), where we observe that a lower value of τ gives better results. In Figure 6(c), we compare the model’s performance using dif- ferent batch-sizes and find that the accuracy first improves and then declines on increasing the batch-size. It is interest- ing to note that our observations on projection dimension and temperature are in-line with those reported in [3, 12]. However, we observe a decline in performance on increas- ing the batch-size from 12 to 16. We believe this is because of the characteristics of this dataset where we have only two categories to distinguish (ASD and Control), and a medium batch-size of 12 is possibly optimum to segregate a group of similar and dissimilar pairs in a relative manner.
4.7.	Qualitative Results
In Figure 7, we compare the predictions made by the two top-performing methods SSCL and MSupCL on examples from the Hand Gesture dataset, for both ASD and Control categories. Along with each example, we also show the confidence score of these methods, and whether the predic- tion was correct/incorrect. Here, the first row shows exam- ples that are correctly classified by both the methods, the middle row shows examples that are correctly classified by MSupCL but misclassified by SSCL, and the last row shows examples that are misclassified by both the methods. In gen-

eral, we observe that MSupCL has a relatively high con- fidence score in cases where it makes correct predictions, and a low (near chance) confidence when it makes an incor- rect prediction. On the other hand, SSCL has a relatively low confidence score in cases where it makes correct pre- dictions, and a high confidence when it makes an incorrect prediction. Overall, these results validate the promise of the presented MSupCL approach on this challenging task.
5.	Summary and Conclusion
Automated ASD diagnosis is a challenging and long- standing research problem. Over the last few years, dif- ferent groups of researchers have independently collected various datasets to demonstrate the effectiveness of existing machine learning techniques on this task. These datasets generally contain a small number (a few hundreds) of sam- ples having low inter-class and high intra-class variability. In this paper, we have made an attempt towards addressing these challenges by integrating knowledge from two inde- pendently collected and significantly diverse video datasets in a contrastive learning set-up. To do so, we have pre- sented a multi-dataset supervised contrastive learning tech- nique, and empirically demonstrated its superiority over the competing techniques such as [3, 12, 20]. On a general note, our experiments demonstrate that contrastive learning tech- niques, that learn discriminative features in a relative man-




Figure 7. Qualitative comparisons from the Hand Gesture dataset. The left and right columns show examples from the ASD and Control categories respectively. Along with each example, we show the confidence score of SSCL [3] and MSupCL (ours).


ner, can be quite beneficial in automating healthcare-related tasks that suffer from the above challenges.
Limitations and Potential Negative Social Impact
While our approach outperforms competing techniques and achieves compelling results given the challenges in- volved in this task, one important limitation of all the exam- ined techniques is their substantially high false-positive and false-negative rates (Figure 5). Because of this, we believe more research efforts will be required to make such systems deployable. Also, since ours is a computational learning based study, our experiments consumed energy produced by burning of fossil fuels and warmed our planet.
Compliance with ethical standards
This research study was conducted retrospectively using human subject data provided by the authors of [32] (Hand

Gesture dataset) and [19] (Autism dataset) via a registration process. No additional ethical approvals were required.
Acknowledgments: The authors would like to thank the Ministry of Education (India) for financial support. YV would like to thank the Department of Science and Tech- nology (India) for the INSPIRE Faculty Award 2017.
References
[1]	Fahd A. Alturki, Majid Aljalal, Akram M. Abdurraqeeb, Khalil Alsharabi, and Abdullrahman A. Al-Shamma’a. Common spatial pattern technique with eeg signals for di- agnosis of autism and epilepsy disorders. IEEE Access, 9:24334–24349, 2021.
[2]	Mehmet Baygin, Sengul Dogan, Turker Tuncer, Prabal Datta Barua, Oliver Faust, N. Arunkumar, Enas W. Abdulhay, Elizabeth Emma Palmer, and U. Rajendra Acharya. Auto-

mated asd detection using hybrid deep lightweight features extracted from EEG signals. Computers in Biology and Medicine, 134:104548, 2021.
[3]	Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learn- ing of visual representations. In International Conference on Machine Learning, 2020.
[4]	Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In IEEE Conference on Computer Vi- sion and Pattern Recognition, 2021.
[5]	Yanbei Chen, Manchen Wang, Abhay Mittal, Zhenlin Xu, Paolo Favaro, Joseph Tighe, and Davide Modolo. Scaledet: A scalable multi-dataset object detector. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.
[6]	Geraldine Dawson, Sara Jane Webb, and James McPartland. Understanding the nature of face processing impairment in autism: Insights from behavioral and electrophysiological studies. Developmental Neuropsychology, 27(3):403–424, 2005.
[7]	Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: A survey. Interna- tional Journal of Computer Vision, 129(6):1789–1819, 2021.
[8]	Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised vi- sual representation learning. In IEEE Conference on Com- puter Vision and Pattern Recognition, 2020.
[9]	Anibal Solon Heinsfeld, Alexandre Rosa Franco, Richard Cameron Craddock, Augusto Buchweitz, and Felipe Meneguzzi. Identification of autism spectrum disor- der using deep learning and the abide dataset. NeuroImage : Clinical, 17:16 – 23, 2018.
[10]	Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[11]	M. Jiang and Q. Zhao. Learning visual attention to identify people with autism spectrum disorder. In IEEE International Conference on Computer Vision, 2017.
[12]	Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In Ad- vances in Neural Information Processing Systems, 2020.
[13]	Yazhou Kong, Jianliang Gao, Yunpei Xu, Yi Pan, Jianxin Wang, and Jin Liu. Classification of autism spectrum dis- order by combining brain connectivity and deep neural net- work classifier. Neurocomputing, 324:63–68, 2019.
[14]	H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human motion recogni- tion. In IEEE International Conference on Computer Vision, 2011.
[15]	Jung Hyuk Lee, Geon Woo Lee, Guiyoung Bong, Hee Jeong Yoo, and Hong Kook Kim. End-to-end model-based detec- tion of infants with autism spectrum disorder using a pre- trained model. Sensors, 23(1):202, 2023.
[16]	Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classification using small train- ing sample size. In IAPR Asian Conference on Pattern Recognition, pages 730–734, 2015.

[17]	Wenbo Liu, Ming Li, and Li Yi. Identifying children with autism spectrum disorder based on their face processing ab- normality: A machine learning framework. Autism Research, 9, 2016.
[18]	MJ Maenner and Kelly A. Shaw. Prevalence and charac- teristics of autism spectrum disorder among children aged 8 years. Report, 2021. Autism and Developmental Disabilities Monitoring Network, 11 Sites, United States, 2018.
[19]	Prashant Pandey, A P Prathosh, Manu Kohli, and Josh Pritchard. Guided weak supervision for action recognition with scarce data to assess skills of children with autism. In AAAI Conference on Artificial Intelligence, 2020.
[20]	Mandela Patrick, Yuki M. Asano, Polina Kuznetsova, Ruth Fong, Joa˜o F. Henriques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transformations. In IEEE International Conference on Com- puter Vision, 2021.
[21]	Mladen Rakic, Mariano Cabezas, Kaisar Kushibar, Arnau Oliver, and Xavier Llado. Improving the detection of autism spectrum disorder by combining structural and functional mri information. NeuroImage: Clinical, 25:102181, 2020.
[22]	Mindi Ruan, Paula J. Webster, Xin Li, and Shuo Wang. Deep neural network reveals the world of autism from a first- person perspective. Autism Research, 14(2):333–342, 2021.
[23]	Zeinab Sherkatghanad, Mohammad Sadegh Akhondzadeh, Soorena Salari, Mariam Zomorodi-Moghadam, Moloud Ab- dar, U. Rajendra Acharya, Reza Khosrowabadi, and V. Salari. Automated detection of autism spectrum disorder using a convolutional neural network. Frontiers in Neuro- science, 13, 2019.
[24]	K. Sun, L. Li, L. Li, N. He, and J. Zhu. Spatial attentional bilinear 3D convolutional network for video-based autism spectrum disorder detection. In IEEE International Confer- ence on Acoustics, Speech and Signal Processing, 2020.
[25]	Md. Nurul Ahad Tawhid, Siuly Siuly, Hua Wang, Frank Whittaker, Kate Wang, and Yanchun Zhang. A spectrogram image based intelligent technique for automatic detection of autism spectrum disorder from eeg. PLOS ONE, 16(6):1–20, 06 2021.
[26]	Yuan Tian, Xiongkuo Min, Guangtao Zhai, and Zhiyong Gao. Video-based early asd detection via temporal pyra- mid networks. IEEE International Conference on Multime- dia and Expo, 2019.
[27]	Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[28]	Constantin Ulrich, Fabian Isensee, Tassilo Wald, Maximil- ian Zenk, Michael Baumgartner, and Klaus H. Maier-Hein. MultiTalent: A multi-dataset approach to medical image seg- mentation. In International Conference on Medical Image Computing and Computer Assisted Intervention, 2023.
[29]	Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579–2605, 2008.
[30]	Shuo Wang, Ming Jiang, Xavier Morin Duchesne, Eliza- beth A. Laugeson, Daniel P. Kennedy, Ralph Adolphs, and

Qi Zhao. Atypical visual saliency in autism spectrum dis- order quantified through model-based eye tracking. Neuron, 88(3):604–616, 2015.
[31]	Hao Zhu, Jun Wang, Yin-Ping Zhao, Minhua Lu, and Jun Shi. Contrastive multi-view composite graph convolutional networks based on contribution learning for autism spectrum disorder classification. IEEE Transactions on Biomedical Engineering, 70(6):1943–1954, 2023.
[32]	A. Zunino, P. Morerio, A. Cavallo, C. Ansuini, J. Podda, F. Battaglia, E. Veneselli, C. Becchio, and V. Murino. Video gesture analysis for autism spectrum disorder detection. In IAPR International Conference on Pattern Recognition, 2018.
